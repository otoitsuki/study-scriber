#!/usr/bin/env python3
"""
éŸ³è¨Šè½‰éŒ„æœå‹™ v2
ä½¿ç”¨å¤§åˆ‡ç‰‡ï¼ˆ10-15ç§’ï¼‰+ ç›´æ¥è½‰æ›çš„æ¶æ§‹ï¼Œé¿å…è¤‡é›œçš„ WebM åˆä½µ
"""

import asyncio
import logging
import subprocess
import tempfile
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional, Any, Set
from uuid import UUID
import json
import os
from asyncio import PriorityQueue, Semaphore

from openai import AsyncAzureOpenAI, RateLimitError
from httpx import Timeout

# Task 5: Prometheus ç›£æ§ä¾è³´
try:
    import prometheus_client as prom
    PROMETHEUS_AVAILABLE = True
except ImportError:
    PROMETHEUS_AVAILABLE = False
    logger.warning("prometheus-client æœªå®‰è£ï¼Œç›£æ§æŒ‡æ¨™å°‡è¢«åœç”¨")

from ..db.database import get_supabase_client
from app.core.config import settings
from app.core.ffmpeg import detect_audio_format
from app.core.webm_header_repairer import WebMHeaderRepairer
from app.ws.transcript_feed import manager as transcript_manager
from app.services.r2_client import R2Client

logger = logging.getLogger(__name__)

# Task 5: Prometheus ç›£æ§æŒ‡æ¨™
if PROMETHEUS_AVAILABLE:
    # è½‰éŒ„è«‹æ±‚è¨ˆæ•¸å™¨
    WHISPER_REQ_TOTAL = prom.Counter(
        "whisper_requests_total",
        "Total Whisper API requests",
        ["status", "deployment"]
    )

    # è½‰éŒ„å»¶é²æŒ‡æ¨™
    WHISPER_LATENCY_SECONDS = prom.Summary(
        "whisper_latency_seconds",
        "Whisper API latency",
        ["deployment"]
    )

    # éšŠåˆ—ç©å£“æŒ‡æ¨™
    WHISPER_BACKLOG_GAUGE = prom.Gauge(
        "whisper_backlog_size",
        "Current queue backlog size"
    )

    # éšŠåˆ—è™•ç†çµ±è¨ˆ
    QUEUE_PROCESSED_TOTAL = prom.Counter(
        "queue_processed_total",
        "Total processed jobs",
        ["status"]
    )

    # éšŠåˆ—ç­‰å¾…æ™‚é–“
    QUEUE_WAIT_SECONDS = prom.Summary(
        "queue_wait_seconds",
        "Time jobs spend waiting in queue"
    )

    # ä½µç™¼è™•ç†æ•¸é‡
    CONCURRENT_JOBS_GAUGE = prom.Gauge(
        "concurrent_transcription_jobs",
        "Current number of concurrent transcription jobs"
    )

    logger.info("ğŸ“Š [Metrics] Prometheus ç›£æ§æŒ‡æ¨™å·²åˆå§‹åŒ–")
else:
    # å¦‚æœ Prometheus ä¸å¯ç”¨ï¼Œå‰µå»ºç©ºçš„ä½”ä½ç¬¦
    class NoOpMetric:
        def inc(self, *args, **kwargs): pass
        def set(self, *args, **kwargs): pass
        def time(self): return self
        def __enter__(self): return self
        def __exit__(self, *args): pass
        def labels(self, *args, **kwargs): return self

    WHISPER_REQ_TOTAL = NoOpMetric()
    WHISPER_LATENCY_SECONDS = NoOpMetric()
    WHISPER_BACKLOG_GAUGE = NoOpMetric()
    QUEUE_PROCESSED_TOTAL = NoOpMetric()
    QUEUE_WAIT_SECONDS = NoOpMetric()
    CONCURRENT_JOBS_GAUGE = NoOpMetric()

# å…¨åŸŸæ•ˆèƒ½ç›£æ§é–‹é—œ
ENABLE_PERFORMANCE_LOGGING = os.getenv("ENABLE_PERFORMANCE_LOGGING", "true").lower() == "true"

# Task 1: å„ªåŒ–çš„ timeout é…ç½®
TIMEOUT = Timeout(connect=5, read=55, write=30, pool=5)

# Task 3: ä½µç™¼æ§åˆ¶èˆ‡ä»»å‹™å„ªå…ˆç´šé…ç½®
MAX_CONCURRENT_TRANSCRIPTIONS = 1  # å–®ä¸¦ç™¼ä¿è­‰é †åº
QUEUE_HIGH_PRIORITY = 0  # é‡è©¦ä»»å‹™é«˜å„ªå…ˆç´š
QUEUE_NORMAL_PRIORITY = 1  # æ­£å¸¸ä»»å‹™
MAX_QUEUE_SIZE = 100  # æœ€å¤§éšŠåˆ—å¤§å°
QUEUE_TIMEOUT_SECONDS = 300  # éšŠåˆ—è¶…æ™‚ï¼ˆ5åˆ†é˜ï¼‰

class PerformanceTimer:
    """æ•ˆèƒ½è¨ˆæ™‚å™¨"""

    def __init__(self, operation_name: str):
        self.operation_name = operation_name
        self.start_time = None
        self.end_time = None

    def __enter__(self):
        self.start_time = time.time()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.end_time = time.time()
        duration = self.get_duration()

        if ENABLE_PERFORMANCE_LOGGING:
            if duration > 1.0:  # è¨˜éŒ„è¶…é1ç§’çš„æ“ä½œ
                logger.warning(f"âš ï¸  {self.operation_name} took {duration:.2f}s (slow)")
            else:
                logger.info(f"â±ï¸  {self.operation_name} completed in {duration:.2f}s")

    def get_duration(self) -> float:
        if self.start_time and self.end_time:
            return self.end_time - self.start_time
        return 0.0

# Task 2: æ™ºèƒ½é »ç‡é™åˆ¶è™•ç†å™¨
class RateLimitHandler:
    """æ™ºèƒ½é »ç‡é™åˆ¶è™•ç†å™¨ - é¿å…éé•·ç­‰å¾…"""

    def __init__(self):
        self._delay = 0
        logger.info("ğŸš¦ [RateLimitHandler] é »ç‡é™åˆ¶è™•ç†å™¨å·²åˆå§‹åŒ–")

    async def wait(self):
        """ç­‰å¾…ç•¶å‰å»¶é²æ™‚é–“"""
        if self._delay:
            logger.info(f"â³ [RateLimitHandler] ç­‰å¾… {self._delay}s é¿å…é »ç‡é™åˆ¶")
            await asyncio.sleep(self._delay)

    def backoff(self):
        """å¢åŠ é€€é¿å»¶é²ï¼ˆæŒ‡æ•¸é€€é¿ï¼Œæœ€å¤§ 60 ç§’ï¼‰"""
        previous_delay = self._delay
        self._delay = min((self._delay or 5) * 2, 60)  # æœ€å¤§ 60 ç§’
        logger.warning(f"ğŸ“ˆ [RateLimitHandler] é€€é¿å»¶é²ï¼š{previous_delay}s â†’ {self._delay}s")

    def reset(self):
        """é‡ç½®å»¶é²ï¼ˆAPI å‘¼å«æˆåŠŸæ™‚ï¼‰"""
        if self._delay > 0:
            logger.info(f"âœ… [RateLimitHandler] é‡ç½®å»¶é²ï¼š{self._delay}s â†’ 0s")
            self._delay = 0

# Task 3: è½‰éŒ„ä»»å‹™ä½‡åˆ—ç®¡ç†å™¨
class TranscriptionQueueManager:
    """å„ªå…ˆç´šéšŠåˆ—ç®¡ç†å™¨ - ç¢ºä¿é †åºè™•ç†ä¸¦é¿å…ç©å£“"""

    def __init__(self):
        # å„ªå…ˆç´šéšŠåˆ— (priority, timestamp, job_data)
        self.queue: PriorityQueue = PriorityQueue(maxsize=MAX_QUEUE_SIZE)
        # ä½µç™¼æ§åˆ¶ä¿¡è™Ÿé‡
        self.semaphore = Semaphore(MAX_CONCURRENT_TRANSCRIPTIONS)
        # Worker ä»»å‹™
        self.workers: list[asyncio.Task] = []
        # Task 4: ç©å£“ç›£æ§ä»»å‹™
        self.backlog_monitor_task: Optional[asyncio.Task] = None
        # çµ±è¨ˆæ•¸æ“š
        self.total_processed = 0
        self.total_failed = 0
        self.total_retries = 0
        # Task 4: ç©å£“é–¾å€¼å’Œç›£æ§é–“éš”
        self.backlog_threshold = 30  # è¶…é 5 åˆ†é˜ç©å£“ (30 ä»»å‹™ Ã— 10ç§’)
        self.monitor_interval = 10  # æ¯ 10 ç§’æª¢æŸ¥ä¸€æ¬¡
        self.last_backlog_alert = 0  # ä¸Šæ¬¡ç©å£“è­¦å ±æ™‚é–“
        self.backlog_alert_cooldown = 60  # ç©å£“è­¦å ±å†·å»æ™‚é–“ï¼ˆç§’ï¼‰
        # é‹è¡Œç‹€æ…‹
        self.is_running = False

        logger.info(f"ğŸ¯ [QueueManager] åˆå§‹åŒ–å®Œæˆï¼šmax_concurrent={MAX_CONCURRENT_TRANSCRIPTIONS}, max_queue={MAX_QUEUE_SIZE}")

    async def start_workers(self, num_workers: int = 2):
        """å•Ÿå‹• Worker ä»»å‹™"""
        if self.is_running:
            logger.warning("âš ï¸ [QueueManager] Workers already running")
            return

        self.is_running = True
        logger.info(f"ğŸš€ [QueueManager] å•Ÿå‹• {num_workers} å€‹ Workers")

        # å•Ÿå‹•å·¥ä½œç·šç¨‹
        for i in range(num_workers):
            worker_task = asyncio.create_task(self._worker(f"Worker-{i+1}"))
            self.workers.append(worker_task)

        # Task 4: å•Ÿå‹•ç©å£“ç›£æ§
        self.backlog_monitor_task = asyncio.create_task(self._backlog_monitor())
        logger.info("ğŸ“Š [QueueManager] ç©å£“ç›£æ§å·²å•Ÿå‹•")

    async def stop_workers(self):
        """åœæ­¢æ‰€æœ‰ Workers"""
        if not self.is_running:
            return

        logger.info("â¹ï¸ [QueueManager] åœæ­¢æ‰€æœ‰ Workers")
        self.is_running = False

        # Task 4: åœæ­¢ç©å£“ç›£æ§
        if self.backlog_monitor_task:
            self.backlog_monitor_task.cancel()
            try:
                await self.backlog_monitor_task
            except asyncio.CancelledError:
                pass

        # å–æ¶ˆæ‰€æœ‰ worker ä»»å‹™
        for worker in self.workers:
            worker.cancel()

        # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
        await asyncio.gather(*self.workers, return_exceptions=True)
        self.workers.clear()

    async def enqueue_job(self, session_id: UUID, chunk_sequence: int, webm_data: bytes, priority: int = QUEUE_NORMAL_PRIORITY):
        """å°‡è½‰éŒ„ä»»å‹™åŠ å…¥éšŠåˆ—"""
        timestamp = time.time()
        job_data = {
            'session_id': session_id,
            'chunk_sequence': chunk_sequence,
            'webm_data': webm_data,
            'timestamp': timestamp,
            'retry_count': 0
        }

        try:
            # ä½¿ç”¨ put_nowait é¿å…é˜»å¡ï¼Œå¦‚æœéšŠåˆ—æ»¿äº†æœƒæ‹‹å‡ºç•°å¸¸
            self.queue.put_nowait((priority, timestamp, job_data))

            # Task 5: æ›´æ–°éšŠåˆ—å¤§å°æŒ‡æ¨™
            queue_size = self.queue.qsize()
            WHISPER_BACKLOG_GAUGE.set(queue_size)

            priority_name = "HIGH" if priority == QUEUE_HIGH_PRIORITY else "NORMAL"
            logger.info(f"ğŸ“¥ [QueueManager] ä»»å‹™å·²å…¥éšŠï¼šsession={session_id}, chunk={chunk_sequence}, priority={priority_name}, queue_size={queue_size}")

        except asyncio.QueueFull:
            logger.error(f"âŒ [QueueManager] éšŠåˆ—å·²æ»¿ ({MAX_QUEUE_SIZE})ï¼Œä¸Ÿæ£„ä»»å‹™ï¼šsession={session_id}, chunk={chunk_sequence}")
            # å¯ä»¥è€ƒæ…®å»£æ’­éšŠåˆ—æ»¿çš„éŒ¯èª¤åˆ°å‰ç«¯
            await self._broadcast_queue_full_error(session_id, chunk_sequence)
            raise Exception(f"Transcription queue is full ({MAX_QUEUE_SIZE}), please try again later")

    async def _worker(self, worker_name: str):
        """Worker å”ç¨‹ - è™•ç†éšŠåˆ—ä¸­çš„ä»»å‹™"""
        logger.info(f"ğŸ‘· [QueueManager] {worker_name} é–‹å§‹å·¥ä½œ")

        while self.is_running:
            try:
                # ç­‰å¾…ä»»å‹™
                try:
                    priority, timestamp, job_data = await asyncio.wait_for(
                        self.queue.get(),
                        timeout=1.0  # 1ç§’è¶…æ™‚ï¼Œè®“ worker èƒ½å®šæœŸæª¢æŸ¥é‹è¡Œç‹€æ…‹
                    )
                except asyncio.TimeoutError:
                    continue  # è¶…æ™‚å¾Œç¹¼çºŒæª¢æŸ¥é‹è¡Œç‹€æ…‹

                # æª¢æŸ¥ä»»å‹™æ˜¯å¦éæœŸ
                age = time.time() - timestamp
                if age > QUEUE_TIMEOUT_SECONDS:
                    logger.warning(f"â° [QueueManager] {worker_name} ä¸Ÿæ£„éæœŸä»»å‹™ï¼šage={age:.1f}s, session={job_data['session_id']}, chunk={job_data['chunk_sequence']}")
                    self.queue.task_done()
                    continue

                # Task 5: è¨˜éŒ„éšŠåˆ—ç­‰å¾…æ™‚é–“
                wait_time = time.time() - timestamp
                QUEUE_WAIT_SECONDS.observe(wait_time)

                # ç²å–ä½µç™¼æ§åˆ¶æ¬Š
                async with self.semaphore:
                    session_id = job_data['session_id']
                    chunk_sequence = job_data['chunk_sequence']

                    logger.info(f"ğŸ”§ [QueueManager] {worker_name} è™•ç†ä»»å‹™ï¼šsession={session_id}, chunk={chunk_sequence}, age={age:.1f}s, wait={wait_time:.1f}s")

                    try:
                        # åŸ·è¡Œè½‰éŒ„
                        success = await self._process_transcription_job(job_data)

                        if success:
                            self.total_processed += 1
                            # Task 5: è¨˜éŒ„æˆåŠŸè™•ç†çš„ä»»å‹™
                            QUEUE_PROCESSED_TOTAL.labels(status="success").inc()
                            logger.info(f"âœ… [QueueManager] {worker_name} ä»»å‹™å®Œæˆï¼šsession={session_id}, chunk={chunk_sequence}")
                        else:
                            # è™•ç†å¤±æ•—ï¼Œæ±ºå®šæ˜¯å¦é‡è©¦
                            # Task 5: è¨˜éŒ„å¤±æ•—è™•ç†çš„ä»»å‹™
                            QUEUE_PROCESSED_TOTAL.labels(status="failed").inc()
                            await self._handle_job_failure(job_data, worker_name)

                    except Exception as e:
                        logger.error(f"ğŸ’¥ [QueueManager] {worker_name} ä»»å‹™ç•°å¸¸ï¼šsession={session_id}, chunk={chunk_sequence}, error={e}")
                        # Task 5: è¨˜éŒ„ç•°å¸¸è™•ç†çš„ä»»å‹™
                        QUEUE_PROCESSED_TOTAL.labels(status="exception").inc()
                        await self._handle_job_failure(job_data, worker_name)

                # æ¨™è¨˜ä»»å‹™å®Œæˆ
                self.queue.task_done()

            except Exception as e:
                logger.error(f"ğŸ’¥ [QueueManager] {worker_name} Worker ç•°å¸¸ï¼š{e}")
                await asyncio.sleep(1)  # çŸ­æš«ä¼‘æ¯å¾Œç¹¼çºŒ

        logger.info(f"ğŸ‘· [QueueManager] {worker_name} åœæ­¢å·¥ä½œ")

    async def _process_transcription_job(self, job_data: dict) -> bool:
        """è™•ç†å–®å€‹è½‰éŒ„ä»»å‹™"""
        session_id = job_data['session_id']
        chunk_sequence = job_data['chunk_sequence']
        webm_data = job_data['webm_data']

        try:
            # ç²å–è½‰éŒ„æœå‹™
            service = await initialize_transcription_service_v2()
            if not service:
                logger.error(f"âŒ [QueueManager] è½‰éŒ„æœå‹™ä¸å¯ç”¨ï¼šsession={session_id}, chunk={chunk_sequence}")
                return False

            # åŸ·è¡Œè½‰éŒ„
            result = await service._transcribe_audio(webm_data, session_id, chunk_sequence)
            if result:
                # å„²å­˜ä¸¦å»£æ’­çµæœ
                await service._save_and_push_result(session_id, chunk_sequence, result)
                return True
            else:
                logger.warning(f"âš ï¸ [QueueManager] è½‰éŒ„ç„¡çµæœï¼šsession={session_id}, chunk={chunk_sequence}")
                return False

        except RateLimitError as e:
            logger.warning(f"ğŸš¦ [QueueManager] é »ç‡é™åˆ¶ï¼šsession={session_id}, chunk={chunk_sequence}, error={e}")
            # 429 éŒ¯èª¤ä¸ç®—å¤±æ•—ï¼Œæœƒè¢«é‡æ–°æ’éšŠ
            raise e
        except Exception as e:
            logger.error(f"âŒ [QueueManager] è½‰éŒ„å¤±æ•—ï¼šsession={session_id}, chunk={chunk_sequence}, error={e}")
            return False

    async def _handle_job_failure(self, job_data: dict, worker_name: str):
        """è™•ç†ä»»å‹™å¤±æ•—"""
        session_id = job_data['session_id']
        chunk_sequence = job_data['chunk_sequence']
        retry_count = job_data.get('retry_count', 0)

        if retry_count < 3:  # æœ€å¤šé‡è©¦ 3 æ¬¡
            job_data['retry_count'] = retry_count + 1
            self.total_retries += 1

            # é‡æ–°æ’éšŠï¼ˆé«˜å„ªå…ˆç´šï¼‰
            await self.enqueue_job(
                session_id,
                chunk_sequence,
                job_data['webm_data'],
                priority=QUEUE_HIGH_PRIORITY
            )

            logger.info(f"ğŸ”„ [QueueManager] {worker_name} é‡æ–°æ’éšŠï¼šsession={session_id}, chunk={chunk_sequence}, retry={retry_count + 1}")
        else:
            self.total_failed += 1
            logger.error(f"âŒ [QueueManager] {worker_name} ä»»å‹™æœ€çµ‚å¤±æ•—ï¼šsession={session_id}, chunk={chunk_sequence}, max_retries_exceeded")

            # å»£æ’­æœ€çµ‚å¤±æ•—é€šçŸ¥
            await self._broadcast_final_failure(session_id, chunk_sequence)

    async def _broadcast_queue_full_error(self, session_id: UUID, chunk_sequence: int):
        """å»£æ’­éšŠåˆ—æ»¿éŒ¯èª¤"""
        try:
            error_data = {
                "type": "transcription_error",
                "error_type": "queue_full",
                "message": f"è½‰éŒ„éšŠåˆ—å·²æ»¿ ({MAX_QUEUE_SIZE})ï¼Œè«‹ç¨å¾Œé‡è©¦",
                "session_id": str(session_id),
                "chunk_sequence": chunk_sequence,
                "timestamp": datetime.utcnow().isoformat()
            }
            await transcript_manager.broadcast(
                json.dumps(error_data),
                str(session_id)
            )
        except Exception as e:
            logger.error(f"Failed to broadcast queue full error: {e}")

    async def _broadcast_final_failure(self, session_id: UUID, chunk_sequence: int):
        """å»£æ’­æœ€çµ‚å¤±æ•—é€šçŸ¥"""
        try:
            error_data = {
                "type": "transcription_error",
                "error_type": "final_failure",
                "message": f"æ®µè½ {chunk_sequence} è½‰éŒ„æœ€çµ‚å¤±æ•—ï¼Œå·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸",
                "session_id": str(session_id),
                "chunk_sequence": chunk_sequence,
                "timestamp": datetime.utcnow().isoformat()
            }
            await transcript_manager.broadcast(
                json.dumps(error_data),
                str(session_id)
            )
        except Exception as e:
            logger.error(f"Failed to broadcast final failure: {e}")

    # Task 4: ç©å£“ç›£æ§å™¨
    async def _backlog_monitor(self):
        """ç©å£“ç›£æ§å”ç¨‹ - å®šæœŸæª¢æŸ¥éšŠåˆ—ç©å£“ä¸¦é€šçŸ¥å‰ç«¯"""
        logger.info("ğŸ“Š [BacklogMonitor] ç©å£“ç›£æ§é–‹å§‹")

        while self.is_running:
            try:
                queue_size = self.queue.qsize()
                current_time = time.time()

                # æª¢æŸ¥æ˜¯å¦è¶…éç©å£“é–¾å€¼
                if queue_size > self.backlog_threshold:
                    # æª¢æŸ¥å†·å»æ™‚é–“ï¼Œé¿å…é »ç¹é€šçŸ¥
                    if current_time - self.last_backlog_alert > self.backlog_alert_cooldown:
                        await self._broadcast_backlog_alert(queue_size)
                        self.last_backlog_alert = current_time
                        logger.warning(f"âš ï¸ [BacklogMonitor] éšŠåˆ—ç©å£“è­¦å ±ï¼šqueue_size={queue_size}, threshold={self.backlog_threshold}")

                # è¨˜éŒ„éšŠåˆ—ç‹€æ…‹ï¼ˆèª¿è©¦ç”¨ï¼‰
                if queue_size > 0:
                    logger.debug(f"ğŸ“Š [BacklogMonitor] éšŠåˆ—ç‹€æ…‹ï¼šsize={queue_size}, processed={self.total_processed}, failed={self.total_failed}")

                # ç­‰å¾…ä¸‹æ¬¡æª¢æŸ¥
                await asyncio.sleep(self.monitor_interval)

            except Exception as e:
                logger.error(f"ğŸ’¥ [BacklogMonitor] ç›£æ§ç•°å¸¸ï¼š{e}")
                await asyncio.sleep(self.monitor_interval)

        logger.info("ğŸ“Š [BacklogMonitor] ç©å£“ç›£æ§åœæ­¢")

    async def _broadcast_backlog_alert(self, queue_size: int):
        """å»£æ’­ç©å£“è­¦å ±åˆ°æ‰€æœ‰æ´»èºæœƒè©±"""
        try:
            # è¨ˆç®—é ä¼°ç­‰å¾…æ™‚é–“
            estimated_wait_minutes = (queue_size * 12) // 60  # å‡è¨­æ¯å€‹ä»»å‹™å¹³å‡ 12 ç§’

            alert_data = {
                "event": "stt_backlog",
                "type": "backlog_alert",
                "queue_size": queue_size,
                "threshold": self.backlog_threshold,
                "estimated_wait_minutes": estimated_wait_minutes,
                "message": f"è½‰éŒ„éšŠåˆ—ç©å£“ï¼š{queue_size} å€‹ä»»å‹™ç­‰å¾…è™•ç†ï¼Œé ä¼°å»¶é² {estimated_wait_minutes} åˆ†é˜",
                "timestamp": datetime.utcnow().isoformat(),
                "level": "warning" if queue_size < self.backlog_threshold * 2 else "critical"
            }

            # å»£æ’­åˆ°æ‰€æœ‰æ´»èºé€£æ¥
            active_connections = getattr(transcript_manager, 'active_connections', {})
            if active_connections:
                broadcast_message = json.dumps(alert_data)

                # å»£æ’­åˆ°æ‰€æœ‰æœƒè©±
                for session_id in list(active_connections.keys()):
                    try:
                        await transcript_manager.broadcast(broadcast_message, session_id)
                    except Exception as e:
                        logger.warning(f"Failed to broadcast backlog alert to session {session_id}: {e}")

                logger.info(f"ğŸ“¢ [BacklogMonitor] ç©å£“è­¦å ±å·²å»£æ’­åˆ° {len(active_connections)} å€‹æœƒè©±")
            else:
                logger.debug("ğŸ“¢ [BacklogMonitor] ç„¡æ´»èºæœƒè©±ï¼Œè·³éç©å£“è­¦å ±å»£æ’­")

        except Exception as e:
            logger.error(f"Failed to broadcast backlog alert: {e}")

    async def _broadcast_queue_recovery(self, queue_size: int):
        """å»£æ’­éšŠåˆ—æ¢å¾©æ­£å¸¸é€šçŸ¥"""
        try:
            recovery_data = {
                "event": "stt_recovery",
                "type": "queue_recovery",
                "queue_size": queue_size,
                "message": f"è½‰éŒ„éšŠåˆ—å·²æ¢å¾©æ­£å¸¸ï¼šç•¶å‰ {queue_size} å€‹ä»»å‹™",
                "timestamp": datetime.utcnow().isoformat(),
                "level": "info"
            }

            # å»£æ’­åˆ°æ‰€æœ‰æ´»èºé€£æ¥
            active_connections = getattr(transcript_manager, 'active_connections', {})
            if active_connections:
                broadcast_message = json.dumps(recovery_data)

                for session_id in list(active_connections.keys()):
                    try:
                        await transcript_manager.broadcast(broadcast_message, session_id)
                    except Exception as e:
                        logger.warning(f"Failed to broadcast recovery to session {session_id}: {e}")

                logger.info(f"ğŸ“¢ [BacklogMonitor] æ¢å¾©é€šçŸ¥å·²å»£æ’­åˆ° {len(active_connections)} å€‹æœƒè©±")

        except Exception as e:
            logger.error(f"Failed to broadcast queue recovery: {e}")

    def get_stats(self) -> dict:
        """ç²å–éšŠåˆ—çµ±è¨ˆä¿¡æ¯"""
        queue_size = self.queue.qsize()
        return {
            'queue_size': queue_size,
            'max_queue_size': MAX_QUEUE_SIZE,
            'total_processed': self.total_processed,
            'total_failed': self.total_failed,
            'total_retries': self.total_retries,
            'workers_count': len(self.workers),
            'is_running': self.is_running,
            # Task 4: ç©å£“ç›£æ§çµ±è¨ˆ
            'backlog_threshold': self.backlog_threshold,
            'is_backlogged': queue_size > self.backlog_threshold,
            'monitor_interval': self.monitor_interval,
            'last_backlog_alert': self.last_backlog_alert,
            'estimated_wait_seconds': queue_size * 12 if queue_size > 0 else 0
        }

# é…ç½®å¸¸æ•¸
CHUNK_DURATION = settings.AUDIO_CHUNK_DURATION_SEC  # å¾é…ç½®è®€å–åˆ‡ç‰‡æ™‚é•·
PROCESSING_TIMEOUT = 30  # è™•ç†è¶…æ™‚ï¼ˆç§’ï¼‰
MAX_RETRIES = 3  # æœ€å¤§é‡è©¦æ¬¡æ•¸

# å…¨åŸŸé›†åˆè¿½è¹¤å·²å»£æ’­ active ç›¸ä½çš„ session
_active_phase_sent: Set[str] = set()

# å…¨åŸŸé »ç‡é™åˆ¶è™•ç†å™¨
rate_limit = RateLimitHandler()

# Task 3: å…¨åŸŸéšŠåˆ—ç®¡ç†å™¨
queue_manager = TranscriptionQueueManager()

class SimpleAudioTranscriptionService:
    """ç°¡åŒ–çš„éŸ³è¨Šè½‰éŒ„æœå‹™"""

    def __init__(self, azure_client: AsyncAzureOpenAI, deployment_name: str):
        self.client = azure_client
        self.deployment_name = deployment_name
        self.processing_tasks: Dict[str, asyncio.Task] = {}

    # def _get_header_repairer(self) -> WebMHeaderRepairer:
    #     """å»¶é²åˆå§‹åŒ– WebM æª”é ­ä¿®å¾©å™¨ - å·²åœç”¨ï¼Œä¸å†éœ€è¦æª”é ­ä¿®å¾©"""
    #     if self._header_repairer is None:
    #         self._header_repairer = WebMHeaderRepairer()
    #     return self._header_repairer

    # def _extract_and_cache_header(self, session_id: str, chunk_0_data: bytes) -> bool:
    #     """æª”é ­æå–å’Œç·©å­˜ - å·²åœç”¨ï¼Œæ¯å€‹ chunk éƒ½æœ‰å®Œæ•´æª”é ­"""
    #     # ä¸å†éœ€è¦ï¼Œæ¯å€‹ chunk éƒ½åŒ…å«å®Œæ•´ WebM Header
    #     return True

    # def _get_cached_header(self, session_id: str) -> Optional[bytes]:
    #     """ç²å–ç·©å­˜æª”é ­ - å·²åœç”¨ï¼Œä¸å†éœ€è¦æª”é ­ç·©å­˜"""
    #     # ä¸å†éœ€è¦ï¼Œæ¯å€‹ chunk éƒ½åŒ…å«å®Œæ•´ WebM Header
    #     return None

    # def _clear_session_cache(self, session_id: str) -> None:
    #     """æ¸…ç†æœƒè©±ç·©å­˜ - å·²åœç”¨"""
    #     # ä¸å†éœ€è¦ç·©å­˜
    #     pass

    # def _cleanup_expired_cache(self) -> None:
    #     """æ¸…ç†éæœŸç·©å­˜ - å·²åœç”¨"""
    #     # ä¸å†éœ€è¦ç·©å­˜ç®¡ç†
    #     pass

    async def process_audio_chunk(self, session_id: UUID, chunk_sequence: int, webm_data: bytes) -> bool:
        """
        è™•ç†å–®ä¸€éŸ³è¨Šåˆ‡ç‰‡ - Task 3: ä½¿ç”¨éšŠåˆ—ç³»çµ±

        Args:
            session_id: æœƒè©± ID
            chunk_sequence: åˆ‡ç‰‡åºè™Ÿ
            webm_data: WebM éŸ³è¨Šæ•¸æ“š

        Returns:
            bool: è™•ç†æ˜¯å¦æˆåŠŸï¼ˆå…¥éšŠæˆåŠŸå³è¦–ç‚ºæˆåŠŸï¼‰
        """
        try:
            logger.info(f"ğŸš€ [TranscriptionService] æäº¤è½‰éŒ„ä»»å‹™ï¼šsession={session_id}, chunk={chunk_sequence}, size={len(webm_data)} bytes")

            # Task 3: å°‡ä»»å‹™æäº¤åˆ°éšŠåˆ—è€Œéç›´æ¥è™•ç†
            await queue_manager.enqueue_job(session_id, chunk_sequence, webm_data)

            # è¿”å› True è¡¨ç¤ºæˆåŠŸæäº¤åˆ°éšŠåˆ—
            return True

        except Exception as e:
            logger.error(f"âŒ [TranscriptionService] æäº¤ä»»å‹™å¤±æ•—ï¼šsession={session_id}, chunk={chunk_sequence}, error={e}")
            return False

    async def _process_chunk_async(self, session_id: UUID, chunk_sequence: int, webm_data: bytes):
        """éåŒæ­¥è™•ç†éŸ³è¨Šåˆ‡ç‰‡ (WebM ç›´æ¥è½‰éŒ„æ¶æ§‹ v2 + æª”é ­ä¿®å¾©)"""
        try:
            with PerformanceTimer(f"Process chunk {chunk_sequence} for session {session_id}"):
                session_id_str = str(session_id)
                logger.info(f"ğŸš€ [WebM ç›´æ¥è½‰éŒ„] é–‹å§‹è™•ç†éŸ³è¨Šåˆ‡ç‰‡ {chunk_sequence} (session: {session_id}, size: {len(webm_data)} bytes)")

                # æ­¥é©Ÿ 1: é©—è­‰å’Œä¿®å¾© WebM æ•¸æ“šï¼ˆæ•´åˆæª”é ­ä¿®å¾©é‚è¼¯ï¼‰
                processed_webm_data = await self._validate_and_repair_webm_data(session_id, chunk_sequence, webm_data)
                if processed_webm_data is None:
                    logger.error(f"âŒ [é©—è­‰å¤±æ•—] Chunk {chunk_sequence} é©—è­‰å¤±æ•—ï¼Œè·³éè™•ç†")
                    return

                # æ­¥é©Ÿ 3: WebM ç›´æ¥è½‰éŒ„ (ä½¿ç”¨ä¿®å¾©å¾Œçš„æ•¸æ“š)
                logger.info(f"âš¡ [æ¶æ§‹å„ªåŒ–] è·³é FFmpeg è½‰æ›ï¼Œç›´æ¥è½‰éŒ„ WebM (chunk {chunk_sequence})")
                transcript_result = await self._transcribe_audio(processed_webm_data, session_id, chunk_sequence)
                if not transcript_result:
                    logger.error(f"Failed to transcribe WebM chunk {chunk_sequence}")
                    return

                # æ­¥é©Ÿ 4: å„²å­˜ä¸¦æ¨é€çµæœ
                await self._save_and_push_result(session_id, chunk_sequence, transcript_result)

                logger.info(f"âœ… æˆåŠŸè™•ç†éŸ³è¨Šåˆ‡ç‰‡ {chunk_sequence}: '{transcript_result.get('text', '')[:50]}...'")

        except Exception as e:
            logger.error(f"Error processing chunk {chunk_sequence} for session {session_id}: {e}", exc_info=True)

    async def _validate_and_repair_webm_data(self, session_id: UUID, chunk_sequence: int, webm_data: bytes) -> Optional[bytes]:
        """
        ç°¡åŒ–çš„ WebM æ•¸æ“šé©—è­‰ï¼ˆå„ªåŒ–å¾Œæ¶æ§‹ï¼‰

        ç”±æ–¼ SegmentedAudioRecorder æ¯å€‹ chunk éƒ½åŒ…å«å®Œæ•´ WebM Headerï¼Œ
        ä¸å†éœ€è¦è¤‡é›œçš„æª”é ­ä¿®å¾©é‚è¼¯ï¼Œåªéœ€åŸºæœ¬é©—è­‰å³å¯ã€‚

        Args:
            session_id: æœƒè©± ID
            chunk_sequence: åˆ‡ç‰‡åºè™Ÿ
            webm_data: åŸå§‹ WebM éŸ³è¨Šæ•¸æ“š

        Returns:
            Optional[bytes]: é©—è­‰å¾Œçš„ WebM æ•¸æ“šï¼Œé©—è­‰å¤±æ•—æ™‚è¿”å› None
        """
        start_time = time.time()

        try:
            # æ­¥é©Ÿ 1: åŸºæœ¬æ•¸æ“šé©—è­‰
            if not webm_data or len(webm_data) < 50:
                logger.warning(f"WebM chunk {chunk_sequence} too small: {len(webm_data) if webm_data else 0} bytes")
                return None

            # æ­¥é©Ÿ 2: ç°¡åŒ–é©—è­‰ - æ¯å€‹ chunk éƒ½æ‡‰è©²æœ‰å®Œæ•´æª”é ­
            logger.debug(f"ğŸ¯ [ç°¡åŒ–é©—è­‰] Chunk {chunk_sequence} æ•¸æ“šå¤§å°: {len(webm_data)} bytes (session: {session_id})")

            # æª¢æŸ¥æ˜¯å¦ç‚º WebM æ ¼å¼ï¼ˆç°¡å–®æª¢æŸ¥ EBML headerï¼‰
            if webm_data[:4] == b'\x1A\x45\xDF\xA3':
                logger.debug(f"âœ… [æª”é ­æª¢æŸ¥] Chunk {chunk_sequence} åŒ…å«å®Œæ•´ WebM EBML header")
            else:
                logger.warning(f"âš ï¸ [æª”é ­æª¢æŸ¥] Chunk {chunk_sequence} å¯èƒ½ä¸æ˜¯æ¨™æº– WebM æ ¼å¼ï¼Œä½†ç¹¼çºŒè™•ç†")

            # æ­¥é©Ÿ 3: æ•ˆèƒ½çµ±è¨ˆ
            total_time = (time.time() - start_time) * 1000  # ms
            logger.debug(f"ğŸ“Š [ç°¡åŒ–è™•ç†] Chunk {chunk_sequence} é©—è­‰å®Œæˆ - ç¸½è¨ˆ: {total_time:.1f}ms")

            # æ•ˆèƒ½è­¦å‘Šï¼ˆæ‡‰è©²å¾ˆå¿«ï¼‰
            if total_time > 10:  # è¶…é10msè­¦å‘Šï¼ˆç°¡åŒ–å¾Œæ‡‰è©²æ›´å¿«ï¼‰
                logger.warning(f"âš ï¸ [æ•ˆèƒ½è­¦å‘Š] Chunk {chunk_sequence} ç°¡åŒ–é©—è­‰æ™‚é–“éé•·: {total_time:.1f}ms")

            return webm_data  # ç›´æ¥è¿”å›åŸå§‹æ•¸æ“š

        except Exception as e:
            logger.error(f"âŒ [ç°¡åŒ–é©—è­‰] Chunk {chunk_sequence} è™•ç†ç•°å¸¸: {e}")
            return webm_data  # é™ç´šä½¿ç”¨åŸå§‹æ•¸æ“š

    async def _convert_webm_to_wav(self, webm_data: bytes, chunk_sequence: int, session_id: UUID) -> Optional[bytes]:
        """
        å°‡ WebM / fMP4 è½‰æ›ç‚º WAV (ä¿ç•™ç”¨æ–¼æœ€çµ‚ä¸‹è¼‰æª”æ¡ˆ)

        æ³¨æ„ï¼šåœ¨ WebM ç›´æ¥è½‰éŒ„æ¶æ§‹ v2 ä¸­ï¼Œæ­¤æ–¹æ³•ä¸å†ç”¨æ–¼å³æ™‚è½‰éŒ„æµç¨‹ï¼Œ
        è€Œæ˜¯ä¿ç•™ä½œç‚ºæœ€çµ‚åŒ¯å‡ºæ™‚ç”Ÿæˆ WAV æª”æ¡ˆçš„å‚™é¸æ–¹æ¡ˆã€‚
        """

        async def _broadcast_error(error_type: str, error_message: str, details: str = None):
            """é€é WebSocket å»£æ’­éŒ¯èª¤è¨Šæ¯åˆ°å‰ç«¯"""
            try:
                from app.ws.transcript_feed import manager as transcript_manager

                # ç”ŸæˆéŸ³æª”è¨ºæ–·è³‡è¨Š
                hex_header = webm_data[:32].hex(' ', 8).upper() if webm_data else "ç„¡æ•¸æ“š"
                audio_format = detect_audio_format(webm_data)

                # æ ¹æ“šæª¢æ¸¬åˆ°çš„æ ¼å¼æä¾›å»ºè­°
                def get_format_suggestion(audio_format: str) -> str:
                    suggestions = {
                        'fmp4': 'å»ºè­°æª¢æŸ¥ç€è¦½å™¨éŒ„éŸ³è¨­å®šï¼Œæˆ–å˜—è©¦ä½¿ç”¨ WebM æ ¼å¼',
                        'mp4': 'å»ºè­°ç¢ºèªéŸ³æª”å®Œæ•´æ€§ï¼Œæˆ–å˜—è©¦ä½¿ç”¨ WebM æ ¼å¼',
                        'webm': 'å»ºè­°æª¢æŸ¥ WebM ç·¨ç¢¼å™¨è¨­å®š',
                        'unknown': 'å»ºè­°æª¢æŸ¥ç€è¦½å™¨æ˜¯å¦æ”¯æ´éŸ³è¨ŠéŒ„è£½ï¼Œæˆ–å˜—è©¦é‡æ–°æ•´ç†é é¢'
                    }
                    return suggestions.get(audio_format, 'å»ºè­°æª¢æŸ¥éŸ³æª”æ ¼å¼æ˜¯å¦æ”¯æ´')

                error_data = {
                    "type": "conversion_error",
                    "error_type": error_type,
                    "message": error_message,
                    "details": details,
                    "session_id": str(session_id),
                    "chunk_sequence": chunk_sequence,
                    "timestamp": datetime.utcnow().isoformat(),
                    "diagnostics": {
                        "detected_format": audio_format,
                        "file_size": len(webm_data) if webm_data else 0,
                        "header_hex": hex_header,
                        "suggestion": get_format_suggestion(audio_format)
                    }
                }
                await transcript_manager.broadcast(
                    json.dumps(error_data),
                    str(session_id)
                )
                logger.info(f"ğŸš¨ [éŒ¯èª¤å»£æ’­] å·²é€šçŸ¥å‰ç«¯è½‰æ›éŒ¯èª¤: {error_type}")
                logger.debug(f"   - æ ¼å¼è¨ºæ–·: {audio_format}, å¤§å°: {len(webm_data) if webm_data else 0} bytes")
                logger.debug(f"   - é ­éƒ¨æ•¸æ“š: {hex_header}")
            except Exception as e:
                logger.error(f"Failed to broadcast error message: {e}")

        try:
            audio_format = detect_audio_format(webm_data)
            logger.info(f"ğŸµ [æ ¼å¼æª¢æ¸¬] æª¢æ¸¬åˆ°éŸ³æª”æ ¼å¼: {audio_format} (chunk {chunk_sequence}, å¤§å°: {len(webm_data)} bytes)")

            with PerformanceTimer(f"{audio_format.upper()} to WAV conversion for chunk {chunk_sequence}"):

                # åŸºæœ¬ FFmpeg åƒæ•¸
                cmd = ['ffmpeg']

                # ä¾ä¾†æºæ ¼å¼æ±ºå®šè¼¸å…¥åƒæ•¸
                if audio_format == 'mp4':
                    # Safari ç”¢å‡ºçš„ fragmented MP4 - è®“ FFmpeg è‡ªå‹•æª¢æ¸¬æ ¼å¼
                    # ä¸æŒ‡å®š -f åƒæ•¸ï¼Œèƒ½æ›´å¥½è™•ç†å„ç¨® MP4 è®Šé«”
                    pass
                elif audio_format == 'webm':
                    cmd += ['-f', 'webm']
                elif audio_format == 'ogg':
                    cmd += ['-f', 'ogg']
                elif audio_format == 'wav':
                    cmd += ['-f', 'wav']

                # é€šç”¨æ——æ¨™ï¼šç”Ÿæˆæ™‚é–“æˆ³è™•ç†ä¸å®Œæ•´æµ
                cmd += ['-fflags', '+genpts', '-i', 'pipe:0', '-ac', '1', '-ar', '16000', '-f', 'wav', '-y', 'pipe:1']

                logger.debug(f"ğŸ”§ [FFmpeg] åŸ·è¡Œå‘½ä»¤: {' '.join(cmd)}")

                process = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdin=asyncio.subprocess.PIPE,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )

                stdout, stderr = await asyncio.wait_for(
                    process.communicate(input=webm_data),
                    timeout=PROCESSING_TIMEOUT
                )

                if process.returncode != 0:
                    error_msg = stderr.decode('utf-8', errors='ignore') if stderr else "Unknown error"
                    logger.error(f"âŒ [FFmpeg éŒ¯èª¤] è½‰æ›å¤±æ•— chunk {chunk_sequence}")
                    logger.error(f"   - æ ¼å¼: {audio_format}")
                    logger.error(f"   - è¿”å›ç¢¼: {process.returncode}")
                    logger.error(f"   - éŒ¯èª¤è¨Šæ¯: {error_msg}")
                    logger.error(f"   - è¼¸å…¥å¤§å°: {len(webm_data)} bytes")

                    # å¢å¼·éŒ¯èª¤åˆ†æï¼Œç‰¹åˆ¥é‡å° fragmented MP4 éŒ¯èª¤
                    if "could not find corresponding trex" in error_msg.lower():
                        error_reason = "Fragmented MP4 æ ¼å¼éŒ¯èª¤ï¼šç¼ºå°‘ Track Extends (trex) ç›’ï¼Œéœ€è¦ä½¿ç”¨ç‰¹æ®Šçš„ movflags åƒæ•¸"
                        detailed_suggestion = (
                            "ğŸ”§ è§£æ±ºæ–¹æ¡ˆï¼š\n"
                            "1. æª¢æ¸¬åˆ° fragmented MP4 æ ¼å¼ï¼Œå»ºè­°é‡æ–°æ•´ç†é é¢\n"
                            "2. å¦‚æœå•é¡ŒæŒçºŒï¼Œè«‹å˜—è©¦ä½¿ç”¨ä¸åŒç€è¦½å™¨\n"
                            "3. Safari ç”¨æˆ¶å»ºè­°åˆ‡æ›è‡³ Chrome æˆ– Firefox"
                        )
                    elif "trun track id unknown" in error_msg.lower():
                        error_reason = "Fragmented MP4 è¿½è¹¤ ID éŒ¯èª¤ï¼šTrack Run (trun) ç›’ä¸­çš„è»Œé“ ID ç„¡æ³•è­˜åˆ¥"
                        detailed_suggestion = (
                            "ğŸ”§ è§£æ±ºæ–¹æ¡ˆï¼š\n"
                            "1. é€™æ˜¯ fragmented MP4 ç‰¹æœ‰éŒ¯èª¤\n"
                            "2. å»ºè­°é‡æ–°éŒ„éŸ³æˆ–é‡å•Ÿç€è¦½å™¨\n"
                            "3. è€ƒæ…®é™ä½éŒ„éŸ³å“è³ªè¨­å®š"
                        )
                    elif "Invalid data found when processing input" in error_msg:
                        error_reason = f"éŸ³æª”æ ¼å¼ {audio_format} èˆ‡ FFmpeg ä¸å…¼å®¹ï¼Œå¯èƒ½æ˜¯ç·¨ç¢¼å•é¡Œ"
                        detailed_suggestion = (
                            "ğŸ”§ è§£æ±ºæ–¹æ¡ˆï¼š\n"
                            "1. æª¢æŸ¥éŸ³æª”æ˜¯å¦å®Œæ•´ä¸‹è¼‰\n"
                            "2. ç¢ºèªç€è¦½å™¨éŒ„éŸ³æ ¼å¼è¨­å®š\n"
                            "3. å˜—è©¦é‡æ–°é–‹å§‹éŒ„éŸ³"
                        )
                    elif "No such file or directory" in error_msg:
                        error_reason = "FFmpeg ç¨‹å¼æœªæ‰¾åˆ°æˆ–é…ç½®éŒ¯èª¤"
                        detailed_suggestion = (
                            "ğŸ”§ è§£æ±ºæ–¹æ¡ˆï¼š\n"
                            "1. è«‹è¯ç¹«æŠ€è¡“æ”¯æ´\n"
                            "2. é€™æ˜¯ä¼ºæœå™¨é…ç½®å•é¡Œ"
                        )
                    elif "Permission denied" in error_msg:
                        error_reason = "FFmpeg æ¬Šé™ä¸è¶³"
                        detailed_suggestion = (
                            "ğŸ”§ è§£æ±ºæ–¹æ¡ˆï¼š\n"
                            "1. è«‹è¯ç¹«æŠ€è¡“æ”¯æ´\n"
                            "2. é€™æ˜¯ä¼ºæœå™¨æ¬Šé™å•é¡Œ"
                        )
                    else:
                        error_reason = f"FFmpeg è™•ç† {audio_format} æ ¼å¼æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤"
                        detailed_suggestion = (
                            "ğŸ”§ è§£æ±ºæ–¹æ¡ˆï¼š\n"
                            "1. å˜—è©¦é‡æ–°éŒ„éŸ³\n"
                            "2. æª¢æŸ¥ç¶²è·¯é€£ç·šæ˜¯å¦ç©©å®š\n"
                            "3. å¦‚æœå•é¡ŒæŒçºŒï¼Œè«‹è¯ç¹«æŠ€è¡“æ”¯æ´"
                        )

                    # è¨˜éŒ„è©³ç´°è¨ºæ–·è³‡è¨Š
                    logger.error(f"   - è¨ºæ–·çµæœ: {error_reason}")
                    logger.error(f"   - å»ºè­°æ–¹æ¡ˆ: {detailed_suggestion}")

                    await _broadcast_error("ffmpeg_conversion_failed", error_reason, detailed_suggestion)
                    return None

                if not stdout or len(stdout) < 100:
                    error_msg = f"FFmpeg ç”¢ç”Ÿçš„ WAV æ•¸æ“šä¸è¶³: {len(stdout) if stdout else 0} bytes"
                    logger.error(f"âŒ [FFmpeg è­¦å‘Š] {error_msg}")
                    await _broadcast_error("insufficient_output", "è½‰æ›å¾Œçš„éŸ³æª”æ•¸æ“šä¸è¶³ï¼Œå¯èƒ½æ˜¯éœéŸ³æˆ–æå£", error_msg)
                    return None

                logger.info(f"âœ… [FFmpeg æˆåŠŸ] {audio_format.upper()} ({len(webm_data)} bytes) â†’ WAV ({len(stdout)} bytes)")
                return stdout

        except asyncio.TimeoutError:
            error_msg = f"FFmpeg è½‰æ›è¶…æ™‚ (>{PROCESSING_TIMEOUT}ç§’)"
            logger.error(f"â° [FFmpeg è¶…æ™‚] {error_msg}")
            await _broadcast_error("conversion_timeout", "éŸ³æª”è½‰æ›è™•ç†æ™‚é–“éé•·", error_msg)
            return None
        except Exception as e:
            error_msg = f"FFmpeg è½‰æ›ç•°å¸¸: {str(e)}"
            logger.error(f"ğŸ’¥ [FFmpeg ç•°å¸¸] {error_msg}")
            await _broadcast_error("conversion_exception", "éŸ³æª”è½‰æ›éç¨‹ä¸­ç™¼ç”Ÿç•°å¸¸éŒ¯èª¤", error_msg)
            return None

    async def _transcribe_audio(self, webm_data: bytes, session_id: UUID, chunk_sequence: int) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨ Azure OpenAI Whisper ç›´æ¥è½‰éŒ„ WebM éŸ³è¨Š (æ¶æ§‹å„ªåŒ– v2 + æ™ºèƒ½é »ç‡é™åˆ¶è™•ç†)"""

        # Task 2: æ™ºèƒ½é »ç‡é™åˆ¶è™•ç† - ç­‰å¾…ç•¶å‰å»¶é²
        await rate_limit.wait()

        # Task 5: è¨˜éŒ„ä½µç™¼è™•ç†æ•¸é‡
        CONCURRENT_JOBS_GAUGE.inc()

        try:
            # Task 5: ç›£æ§è½‰éŒ„å»¶é²
            with WHISPER_LATENCY_SECONDS.labels(deployment=self.deployment_name).time():
                with PerformanceTimer(f"Whisper WebM transcription for chunk {chunk_sequence}"):
                    # å»ºç«‹ WebM æ ¼å¼è‡¨æ™‚æª”æ¡ˆ (ç„¡éœ€ FFmpeg è½‰æ›)
                    with tempfile.NamedTemporaryFile(suffix='.webm', delete=False) as temp_file:
                        temp_file.write(webm_data)
                        temp_file.flush()

                        try:
                            # Task 1: ä½¿ç”¨ç•°æ­¥å®¢æˆ¶ç«¯ç›´æ¥å‘¼å« Whisper API
                            with open(temp_file.name, 'rb') as audio_file:
                                transcript = await self.client.audio.transcriptions.create(
                                    model=self.deployment_name,
                                    file=audio_file,
                                    language="zh",
                                    response_format="text"
                                )

                            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
                            Path(temp_file.name).unlink(missing_ok=True)

                            if not transcript or not transcript.strip():
                                logger.debug(f"Empty transcript for chunk {chunk_sequence}")
                                # Task 5: è¨˜éŒ„ç©ºè½‰éŒ„
                                WHISPER_REQ_TOTAL.labels(status="empty", deployment=self.deployment_name).inc()
                                return None

                            # Task 2: API å‘¼å«æˆåŠŸï¼Œé‡ç½®é »ç‡é™åˆ¶å»¶é²
                            rate_limit.reset()

                            # Task 5: è¨˜éŒ„æˆåŠŸçš„è½‰éŒ„è«‹æ±‚
                            WHISPER_REQ_TOTAL.labels(status="success", deployment=self.deployment_name).inc()

                            logger.info(f"ğŸ¯ [WebM ç›´æ¥è½‰éŒ„] æˆåŠŸè™•ç† chunk {chunk_sequence} (æ ¼å¼: WebM â†’ Whisper API)")

                            return {
                                'text': transcript.strip(),
                                'chunk_sequence': chunk_sequence,
                                'session_id': str(session_id),
                                'timestamp': datetime.utcnow().isoformat(),
                                'language': 'zh-TW',
                                'duration': CHUNK_DURATION
                            }

                        finally:
                            # ç¢ºä¿æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
                            Path(temp_file.name).unlink(missing_ok=True)

        except RateLimitError as e:
            # Task 2: æ™ºèƒ½è™•ç† 429 éŒ¯èª¤
            logger.warning(f"ğŸš¦ [é »ç‡é™åˆ¶] Chunk {chunk_sequence} é‡åˆ° 429 éŒ¯èª¤ï¼š{str(e)}")
            rate_limit.backoff()

            # Task 5: è¨˜éŒ„ 429 éŒ¯èª¤
            WHISPER_REQ_TOTAL.labels(status="rate_limit", deployment=self.deployment_name).inc()

            # å»£æ’­é »ç‡é™åˆ¶éŒ¯èª¤åˆ°å‰ç«¯
            await self._broadcast_transcription_error(
                session_id,
                chunk_sequence,
                "rate_limit_error",
                f"API é »ç‡é™åˆ¶ï¼Œå°‡åœ¨ {rate_limit._delay}s å¾Œé‡è©¦"
            )
            return None

        except Exception as e:
            logger.error(f"WebM direct transcription failed for chunk {chunk_sequence}: {e}")

            # Task 5: è¨˜éŒ„å¤±æ•—çš„è½‰éŒ„è«‹æ±‚
            WHISPER_REQ_TOTAL.labels(status="error", deployment=self.deployment_name).inc()

            # å»£æ’­ Whisper API éŒ¯èª¤åˆ°å‰ç«¯
            await self._broadcast_transcription_error(session_id, chunk_sequence, "whisper_api_error", f"Azure OpenAI Whisper WebM è½‰éŒ„å¤±æ•—: {str(e)}")
            return None

        finally:
            # Task 5: æ¸›å°‘ä½µç™¼è™•ç†æ•¸é‡
            CONCURRENT_JOBS_GAUGE.dec()

    async def _save_and_push_result(self, session_id: UUID, chunk_sequence: int, transcript_result: Dict[str, Any]):
        """å„²å­˜è½‰éŒ„çµæœä¸¦æ¨é€åˆ°å‰ç«¯"""
        try:
            # å„²å­˜åˆ°è³‡æ–™åº«
            supabase = get_supabase_client()

            segment_data = {
                "session_id": str(session_id),
                "chunk_sequence": chunk_sequence,
                "text": transcript_result['text'],
                "start_time": chunk_sequence * CHUNK_DURATION,
                "end_time": (chunk_sequence + 1) * CHUNK_DURATION,
                "confidence": 1.0,
                "language": transcript_result.get('language', 'zh-TW'),
                "created_at": transcript_result['timestamp']
            }

            response = supabase.table("transcript_segments").insert(segment_data).execute()

            if response.data:
                segment_id = response.data[0]['id']
                logger.debug(f"Saved transcript segment {segment_id} for chunk {chunk_sequence}")

                # é€é WebSocket å»£æ’­è½‰éŒ„çµæœ
                # è‹¥å°šæœªå»£æ’­ active ç›¸ä½ï¼Œå…ˆé€å‡º
                if str(session_id) not in _active_phase_sent:
                    logger.info(f"ğŸš€ [è½‰éŒ„æ¨é€] é¦–æ¬¡å»£æ’­ active ç›¸ä½åˆ° session {session_id}")
                    await transcript_manager.broadcast(
                        json.dumps({"phase": "active"}),
                        str(session_id)
                    )
                    _active_phase_sent.add(str(session_id))
                    logger.info(f"âœ… [è½‰éŒ„æ¨é€] Active ç›¸ä½å»£æ’­å®Œæˆ for session {session_id}")

                # æ§‹å»ºé€å­—ç¨¿ç‰‡æ®µè¨Šæ¯
                transcript_message = {
                    "type": "transcript_segment",
                    "session_id": str(session_id),
                    "segment_id": segment_id,
                    "text": transcript_result['text'],
                    "chunk_sequence": chunk_sequence,
                    "start_sequence": chunk_sequence,  # æ·»åŠ  start_sequence æ¬„ä½
                    "start_time": segment_data['start_time'],
                    "end_time": segment_data['end_time'],
                    "confidence": segment_data['confidence'],
                    "timestamp": segment_data['created_at']
                }

                logger.info(f"ğŸ“¡ [è½‰éŒ„æ¨é€] å»£æ’­é€å­—ç¨¿ç‰‡æ®µåˆ° session {session_id}:")
                logger.info(f"   - æ–‡å­—: '{transcript_result['text'][:50]}{'...' if len(transcript_result['text']) > 50 else ''}'")
                logger.info(f"   - åºè™Ÿ: {chunk_sequence}")
                logger.info(f"   - æ™‚é–“: {segment_data['start_time']}s - {segment_data['end_time']}s")

                await transcript_manager.broadcast(
                    json.dumps(transcript_message),
                    str(session_id)
                )

                logger.info(f"âœ… [è½‰éŒ„æ¨é€] é€å­—ç¨¿ç‰‡æ®µå»£æ’­å®Œæˆ for session {session_id}")

                # å»£æ’­è½‰éŒ„å®Œæˆæ¶ˆæ¯
                logger.info(f"å»£æ’­è½‰éŒ„å®Œæˆè¨Šæ¯åˆ° session {session_id}")
                await transcript_manager.broadcast(
                    json.dumps({
                        "type": "transcript_complete",
                        "session_id": str(session_id),
                        "message": "Transcription completed for the batch."
                    }),
                    str(session_id)
                )
                logger.info(f"è½‰éŒ„ä»»å‹™å®Œæˆ for session: {session_id}, chunk: {chunk_sequence}")

        except Exception as e:
            logger.error(f"Failed to save/push transcript for chunk {chunk_sequence}: {e}")
            # å»£æ’­è½‰éŒ„å¤±æ•—éŒ¯èª¤åˆ°å‰ç«¯
            await self._broadcast_transcription_error(session_id, chunk_sequence, "database_error", f"è³‡æ–™åº«æ“ä½œå¤±æ•—: {str(e)}")

    async def _broadcast_transcription_error(self, session_id: UUID, chunk_sequence: int, error_type: str, error_message: str):
        """å»£æ’­è½‰éŒ„éŒ¯èª¤åˆ°å‰ç«¯"""
        try:
            from app.ws.transcript_feed import manager as transcript_manager
            error_data = {
                "type": "transcription_error",
                "error_type": error_type,
                "message": error_message,
                "session_id": str(session_id),
                "chunk_sequence": chunk_sequence,
                "timestamp": datetime.utcnow().isoformat()
            }
            await transcript_manager.broadcast(
                json.dumps(error_data),
                str(session_id)
            )
            logger.info(f"ğŸš¨ [è½‰éŒ„éŒ¯èª¤å»£æ’­] å·²é€šçŸ¥å‰ç«¯è½‰éŒ„éŒ¯èª¤: {error_type}")
        except Exception as e:
            logger.error(f"Failed to broadcast transcription error: {e}")

    # TODO: åœ¨æ­¤è™•å¯¦ç¾æ›´å„ªé›…çš„é—œé–‰é‚è¼¯
    logger.info("Transcription service is shutting down...")

# ----------------------
# å…¼å®¹èˆŠæ¸¬è©¦çš„å·¥å» å‡½å¼èˆ‡å…¨åŸŸè®Šæ•¸
# ----------------------

_transcription_service_v2: Optional[SimpleAudioTranscriptionService] = None


def get_azure_openai_client() -> Optional[AsyncAzureOpenAI]:
    """Task 1: å»ºç«‹ç•°æ­¥ AzureOpenAI ç”¨æˆ¶ç«¯ï¼ŒåŒ…å«å„ªåŒ–çš„ timeout å’Œé‡è©¦é…ç½®"""
    api_key = os.getenv("AZURE_OPENAI_API_KEY")
    endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
    if not api_key or not endpoint:
        logger.warning("âš ï¸ [å®¢æˆ¶ç«¯åˆå§‹åŒ–] Azure OpenAI ç’°å¢ƒè®Šæ•¸ç¼ºå¤±")
        return None

    # Task 1: å‰µå»ºç•°æ­¥å®¢æˆ¶ç«¯ï¼ŒåŒ…å« timeout å’Œæ¸›å°‘é‡è©¦æ¬¡æ•¸
    client = AsyncAzureOpenAI(
        api_key=api_key,
        azure_endpoint=endpoint,
        api_version="2024-06-01",
        timeout=TIMEOUT,
        max_retries=2,  # ç”± 5 æ¬¡é™åˆ° 2 æ¬¡ï¼Œé¿å…ç©å£“
    )

    logger.info("âœ… [å®¢æˆ¶ç«¯åˆå§‹åŒ–] AsyncAzureOpenAI å®¢æˆ¶ç«¯å·²å‰µå»º")
    logger.info(f"   - Timeout: connect={TIMEOUT.connect}s, read={TIMEOUT.read}s")
    logger.info(f"   - Max retries: 2 (å„ªåŒ–å¾Œ)")

    return client


def get_whisper_deployment_name() -> Optional[str]:
    """å–å¾— Whisper éƒ¨ç½²åç¨±ï¼Œç’°å¢ƒè®Šæ•¸ç¼ºå€¼æ™‚å›å‚³ Noneã€‚"""
    return os.getenv("WHISPER_DEPLOYMENT_NAME")


async def initialize_transcription_service_v2() -> Optional[SimpleAudioTranscriptionService]:
    """åˆå§‹åŒ–ä¸¦å¿«å– SimpleAudioTranscriptionService å¯¦ä¾‹ã€‚è‹¥è¨­å®šä¸è¶³å‰‡å›å‚³ Noneã€‚"""
    global _transcription_service_v2
    if _transcription_service_v2 is not None:
        return _transcription_service_v2

    client = get_azure_openai_client()
    deployment = get_whisper_deployment_name()
    if not client or not deployment:
        logger.warning("Azure OpenAI è¨­å®šä¸è¶³ï¼Œç„¡æ³•åˆå§‹åŒ–è½‰éŒ„æœå‹™ v2")
        return None

    _transcription_service_v2 = SimpleAudioTranscriptionService(client, deployment)
    logger.info("âœ… Transcription service v2 initialized with async client")
    return _transcription_service_v2


def cleanup_transcription_service_v2():
    """æ¸…ç†å…¨åŸŸè½‰éŒ„æœå‹™å¯¦ä¾‹ã€‚"""
    global _transcription_service_v2
    _transcription_service_v2 = None
