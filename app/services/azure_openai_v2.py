#!/usr/bin/env python3
"""
éŸ³è¨Šè½‰éŒ„æœå‹™ v2
ä½¿ç”¨å¤§åˆ‡ç‰‡ï¼ˆ10-15ç§’ï¼‰+ ç›´æ¥è½‰æ›çš„æ¶æ§‹ï¼Œé¿å…è¤‡é›œçš„ WebM åˆä½µ
"""

import asyncio
import logging
import subprocess
import tempfile
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional, Any, Set
from uuid import UUID
import json
import os

from openai import AzureOpenAI

from ..db.database import get_supabase_client
from app.core.config import settings
from app.ws.transcript_feed import manager as transcript_manager
from app.services.r2_client import R2Client

logger = logging.getLogger(__name__)

# å…¨åŸŸæ•ˆèƒ½ç›£æ§é–‹é—œ
ENABLE_PERFORMANCE_LOGGING = os.getenv("ENABLE_PERFORMANCE_LOGGING", "true").lower() == "true"

class PerformanceTimer:
    """æ•ˆèƒ½è¨ˆæ™‚å™¨"""

    def __init__(self, operation_name: str):
        self.operation_name = operation_name
        self.start_time = None
        self.end_time = None

    def __enter__(self):
        self.start_time = time.time()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.end_time = time.time()
        duration = self.get_duration()

        if ENABLE_PERFORMANCE_LOGGING:
            if duration > 1.0:  # è¨˜éŒ„è¶…é1ç§’çš„æ“ä½œ
                logger.warning(f"âš ï¸  {self.operation_name} took {duration:.2f}s (slow)")
            else:
                logger.info(f"â±ï¸  {self.operation_name} completed in {duration:.2f}s")

    def get_duration(self) -> float:
        if self.start_time and self.end_time:
            return self.end_time - self.start_time
        return 0.0

# é…ç½®å¸¸æ•¸
CHUNK_DURATION = 12  # 12 ç§’åˆ‡ç‰‡
PROCESSING_TIMEOUT = 30  # è™•ç†è¶…æ™‚ï¼ˆç§’ï¼‰
MAX_RETRIES = 3  # æœ€å¤§é‡è©¦æ¬¡æ•¸

# å…¨åŸŸé›†åˆè¿½è¹¤å·²å»£æ’­ active ç›¸ä½çš„ session
_active_phase_sent: Set[str] = set()

class SimpleAudioTranscriptionService:
    """ç°¡åŒ–çš„éŸ³è¨Šè½‰éŒ„æœå‹™"""

    def __init__(self, azure_client: AzureOpenAI, deployment_name: str):
        self.client = azure_client
        self.deployment_name = deployment_name
        self.processing_tasks: Dict[str, asyncio.Task] = {}

    async def process_audio_chunk(self, session_id: UUID, chunk_sequence: int, webm_data: bytes) -> bool:
        """
        è™•ç†å–®ä¸€éŸ³è¨Šåˆ‡ç‰‡

        Args:
            session_id: æœƒè©± ID
            chunk_sequence: åˆ‡ç‰‡åºè™Ÿ
            webm_data: WebM éŸ³è¨Šæ•¸æ“š

        Returns:
            bool: è™•ç†æ˜¯å¦æˆåŠŸ
        """
        task_key = f"{session_id}_{chunk_sequence}"

        # é¿å…é‡è¤‡è™•ç†
        if task_key in self.processing_tasks:
            logger.debug(f"Chunk {chunk_sequence} already being processed for session {session_id}")
            return False

        # å»ºç«‹è™•ç†ä»»å‹™
        task = asyncio.create_task(
            self._process_chunk_async(session_id, chunk_sequence, webm_data)
        )
        self.processing_tasks[task_key] = task

        # æ¸…ç†å®Œæˆçš„ä»»å‹™
        task.add_done_callback(lambda t: self.processing_tasks.pop(task_key, None))

        return True

    async def _process_chunk_async(self, session_id: UUID, chunk_sequence: int, webm_data: bytes):
        """éåŒæ­¥è™•ç†éŸ³è¨Šåˆ‡ç‰‡"""
        try:
            with PerformanceTimer(f"Process chunk {chunk_sequence} for session {session_id}"):
                logger.info(f"ğŸµ é–‹å§‹è™•ç†éŸ³è¨Šåˆ‡ç‰‡ {chunk_sequence} (session: {session_id}, size: {len(webm_data)} bytes)")

                # æ­¥é©Ÿ 1: é©—è­‰ WebM æ•¸æ“š
                if not self._validate_webm_data(webm_data, chunk_sequence):
                    return

                # æ­¥é©Ÿ 2: WebM â†’ WAV è½‰æ›
                wav_data = await self._convert_webm_to_wav(webm_data, chunk_sequence)
                if not wav_data:
                    logger.error(f"Failed to convert WebM to WAV for chunk {chunk_sequence}")
                    return

                # æ­¥é©Ÿ 3: Whisper è½‰éŒ„
                transcript_result = await self._transcribe_audio(wav_data, session_id, chunk_sequence)
                if not transcript_result:
                    logger.error(f"Failed to transcribe chunk {chunk_sequence}")
                    return

                # æ­¥é©Ÿ 4: å„²å­˜ä¸¦æ¨é€çµæœ
                await self._save_and_push_result(session_id, chunk_sequence, transcript_result)

                logger.info(f"âœ… æˆåŠŸè™•ç†éŸ³è¨Šåˆ‡ç‰‡ {chunk_sequence}: '{transcript_result.get('text', '')[:50]}...'")

        except Exception as e:
            logger.error(f"Error processing chunk {chunk_sequence} for session {session_id}: {e}", exc_info=True)

    def _validate_webm_data(self, webm_data: bytes, chunk_sequence: int) -> bool:
        """é©—è­‰ WebM æ•¸æ“š - ç°¡åŒ–ç‰ˆæœ¬ï¼Œä¿¡ä»»ç€è¦½å™¨ç”¢ç”Ÿçš„è³‡æ–™"""
        if not webm_data or len(webm_data) < 50:
            logger.warning(f"WebM chunk {chunk_sequence} too small: {len(webm_data) if webm_data else 0} bytes")
            return False

        # ç§»é™¤ EBML æ¨™é ­æª¢æŸ¥ï¼Œä¿¡ä»» MediaRecorder ç”¢ç”Ÿçš„è³‡æ–™
        # FFmpeg æœƒç”¨ -fflags +genpts è™•ç†ä¸å®Œæ•´çš„æµå¼è³‡æ–™
        return True

    async def _convert_webm_to_wav(self, webm_data: bytes, chunk_sequence: int) -> Optional[bytes]:
        """å°‡ WebM / fMP4 è½‰æ›ç‚º WAVï¼Œè‡ªå‹•è¾¨è­˜ä¾†æºæ ¼å¼"""

        def _detect_format(data: bytes) -> str:
            """ç°¡æ˜“æª¢æ¸¬éŸ³è¨Šå°è£æ ¼å¼ (webm / mp4)"""
            if len(data) < 12:
                return 'unknown'
            # WebM (Matroska) ä»¥ EBML header é–‹é ­ 0x1A45DFA3
            if data[0:4] == b'\x1A\x45\xDF\xA3':
                return 'webm'
            # MP4/ISOBMFF å¸¸åœ¨ 4â€“8 byte çœ‹åˆ° 'ftyp'
            if b'ftyp' in data[4:12]:
                return 'mp4'
            return 'unknown'

        try:
            audio_format = _detect_format(webm_data)
            with PerformanceTimer(f"{audio_format.upper()} to WAV conversion for chunk {chunk_sequence}"):

                # åŸºæœ¬ FFmpeg åƒæ•¸
                cmd = ['ffmpeg']

                # ä¾ä¾†æºæ ¼å¼æ±ºå®šè¼¸å…¥åƒæ•¸
                if audio_format == 'mp4':
                    # Safari ç”¢å‡ºçš„ fragmented MP4
                    cmd += ['-f', 'mp4']
                # é€šç”¨æ——æ¨™ï¼šç”Ÿæˆæ™‚é–“æˆ³è™•ç†ä¸å®Œæ•´æµ
                cmd += ['-fflags', '+genpts', '-i', 'pipe:0', '-ac', '1', '-ar', '16000', '-f', 'wav', '-y', 'pipe:1']

                process = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdin=asyncio.subprocess.PIPE,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )

                stdout, stderr = await asyncio.wait_for(
                    process.communicate(input=webm_data),
                    timeout=PROCESSING_TIMEOUT
                )

                if process.returncode != 0:
                    error_msg = stderr.decode('utf-8', errors='ignore') if stderr else "Unknown error"
                    logger.error(f"FFmpeg conversion failed for chunk {chunk_sequence}: {error_msg}")
                    return None

                if not stdout or len(stdout) < 100:
                    logger.error(
                        f"FFmpeg produced insufficient WAV data for chunk {chunk_sequence}: {len(stdout) if stdout else 0} bytes")
                    return None

                logger.debug(
                    f"Successfully converted {audio_format.upper()} ({len(webm_data)} bytes) to WAV ({len(stdout)} bytes)")
                return stdout

        except asyncio.TimeoutError:
            logger.error(f"FFmpeg conversion timeout for chunk {chunk_sequence}")
            return None
        except Exception as e:
            logger.error(f"FFmpeg conversion error for chunk {chunk_sequence}: {e}")
            return None

    async def _transcribe_audio(self, wav_data: bytes, session_id: UUID, chunk_sequence: int) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨ Azure OpenAI Whisper è½‰éŒ„éŸ³è¨Š"""
        try:
            with PerformanceTimer(f"Whisper transcription for chunk {chunk_sequence}"):
                # å»ºç«‹è‡¨æ™‚æª”æ¡ˆ
                with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                    temp_file.write(wav_data)
                    temp_file.flush()

                    try:
                        # Whisper API å‘¼å«
                        with open(temp_file.name, 'rb') as audio_file:
                            transcript = self.client.audio.transcriptions.create(
                                model=self.deployment_name,
                                file=audio_file,
                                language="zh",
                                response_format="text"
                            )

                        # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
                        Path(temp_file.name).unlink(missing_ok=True)

                        if not transcript or not transcript.strip():
                            logger.debug(f"Empty transcript for chunk {chunk_sequence}")
                            return None

                        return {
                            'text': transcript.strip(),
                            'chunk_sequence': chunk_sequence,
                            'session_id': str(session_id),
                            'timestamp': datetime.utcnow().isoformat(),
                            'language': 'zh-TW',
                            'duration': CHUNK_DURATION
                        }

                    finally:
                        # ç¢ºä¿æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
                        Path(temp_file.name).unlink(missing_ok=True)

        except Exception as e:
            logger.error(f"Whisper transcription failed for chunk {chunk_sequence}: {e}")
            return None

    async def _save_and_push_result(self, session_id: UUID, chunk_sequence: int, transcript_result: Dict[str, Any]):
        """å„²å­˜è½‰éŒ„çµæœä¸¦æ¨é€åˆ°å‰ç«¯"""
        try:
            # å„²å­˜åˆ°è³‡æ–™åº«
            supabase = get_supabase_client()

            segment_data = {
                "session_id": str(session_id),
                "chunk_sequence": chunk_sequence,
                "text": transcript_result['text'],
                "start_time": chunk_sequence * CHUNK_DURATION,
                "end_time": (chunk_sequence + 1) * CHUNK_DURATION,
                "confidence": 1.0,
                "language": transcript_result.get('language', 'zh-TW'),
                "created_at": transcript_result['timestamp']
            }

            response = supabase.table("transcript_segments").insert(segment_data).execute()

            if response.data:
                segment_id = response.data[0]['id']
                logger.debug(f"Saved transcript segment {segment_id} for chunk {chunk_sequence}")

                # é€é WebSocket å»£æ’­è½‰éŒ„çµæœ
                # è‹¥å°šæœªå»£æ’­ active ç›¸ä½ï¼Œå…ˆé€å‡º
                if str(session_id) not in _active_phase_sent:
                    logger.info(f"ğŸš€ [è½‰éŒ„æ¨é€] é¦–æ¬¡å»£æ’­ active ç›¸ä½åˆ° session {session_id}")
                    await transcript_manager.broadcast(
                        json.dumps({"phase": "active"}),
                        str(session_id)
                    )
                    _active_phase_sent.add(str(session_id))
                    logger.info(f"âœ… [è½‰éŒ„æ¨é€] Active ç›¸ä½å»£æ’­å®Œæˆ for session {session_id}")

                # æ§‹å»ºé€å­—ç¨¿ç‰‡æ®µè¨Šæ¯
                transcript_message = {
                    "type": "transcript_segment",
                    "session_id": str(session_id),
                    "segment_id": segment_id,
                    "text": transcript_result['text'],
                    "chunk_sequence": chunk_sequence,
                    "start_sequence": chunk_sequence,  # æ·»åŠ  start_sequence æ¬„ä½
                    "start_time": segment_data['start_time'],
                    "end_time": segment_data['end_time'],
                    "confidence": segment_data['confidence'],
                    "timestamp": segment_data['created_at']
                }

                logger.info(f"ğŸ“¡ [è½‰éŒ„æ¨é€] å»£æ’­é€å­—ç¨¿ç‰‡æ®µåˆ° session {session_id}:")
                logger.info(f"   - æ–‡å­—: '{transcript_result['text'][:50]}{'...' if len(transcript_result['text']) > 50 else ''}'")
                logger.info(f"   - åºè™Ÿ: {chunk_sequence}")
                logger.info(f"   - æ™‚é–“: {segment_data['start_time']}s - {segment_data['end_time']}s")

                await transcript_manager.broadcast(
                    json.dumps(transcript_message),
                    str(session_id)
                )

                logger.info(f"âœ… [è½‰éŒ„æ¨é€] é€å­—ç¨¿ç‰‡æ®µå»£æ’­å®Œæˆ for session {session_id}")

                # å»£æ’­è½‰éŒ„å®Œæˆæ¶ˆæ¯
                logger.info(f"å»£æ’­è½‰éŒ„å®Œæˆè¨Šæ¯åˆ° session {session_id}")
                await transcript_manager.broadcast(
                    json.dumps({
                        "type": "transcript_complete",
                        "session_id": str(session_id),
                        "message": "Transcription completed for the batch."
                    }),
                    str(session_id)
                )
                logger.info(f"è½‰éŒ„ä»»å‹™å®Œæˆ for session: {session_id}, chunk: {chunk_sequence}")

        except Exception as e:
            logger.error(f"Failed to save/push transcript for chunk {chunk_sequence}: {e}")

    # TODO: åœ¨æ­¤è™•å¯¦ç¾æ›´å„ªé›…çš„é—œé–‰é‚è¼¯
    logger.info("Transcription service is shutting down...")

# ----------------------
# å…¼å®¹èˆŠæ¸¬è©¦çš„å·¥å» å‡½å¼èˆ‡å…¨åŸŸè®Šæ•¸
# ----------------------

_transcription_service_v2: Optional[SimpleAudioTranscriptionService] = None


def get_azure_openai_client() -> Optional[AzureOpenAI]:
    """æ ¹æ“šç’°å¢ƒè®Šæ•¸å»ºç«‹ AzureOpenAI ç”¨æˆ¶ç«¯ï¼Œç¼ºå€¼æ™‚å›å‚³ Noneã€‚"""
    api_key = os.getenv("AZURE_OPENAI_API_KEY")
    endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
    if not api_key or not endpoint:
        return None
    # ä½¿ç”¨é è¨­ API ç‰ˆæœ¬å³å¯
    return AzureOpenAI(api_key=api_key, api_version="2024-06-01", azure_endpoint=endpoint)


def get_whisper_deployment_name() -> Optional[str]:
    """å–å¾— Whisper éƒ¨ç½²åç¨±ï¼Œç’°å¢ƒè®Šæ•¸ç¼ºå€¼æ™‚å›å‚³ Noneã€‚"""
    return os.getenv("WHISPER_DEPLOYMENT_NAME")


async def initialize_transcription_service_v2() -> Optional[SimpleAudioTranscriptionService]:
    """åˆå§‹åŒ–ä¸¦å¿«å– SimpleAudioTranscriptionService å¯¦ä¾‹ã€‚è‹¥è¨­å®šä¸è¶³å‰‡å›å‚³ Noneã€‚"""
    global _transcription_service_v2
    if _transcription_service_v2 is not None:
        return _transcription_service_v2

    client = get_azure_openai_client()
    deployment = get_whisper_deployment_name()
    if not client or not deployment:
        logger.warning("Azure OpenAI è¨­å®šä¸è¶³ï¼Œç„¡æ³•åˆå§‹åŒ–è½‰éŒ„æœå‹™ v2")
        return None

    _transcription_service_v2 = SimpleAudioTranscriptionService(client, deployment)
    logger.info("âœ… Transcription service v2 initialized")
    return _transcription_service_v2


def cleanup_transcription_service_v2():
    """æ¸…ç†å…¨åŸŸè½‰éŒ„æœå‹™å¯¦ä¾‹ã€‚"""
    global _transcription_service_v2
    _transcription_service_v2 = None
