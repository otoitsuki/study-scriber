import os
import asyncio
import logging
import time
from typing import List, Optional, Dict, Any
from uuid import UUID
from datetime import datetime
from io import BytesIO
from decimal import Decimal

from openai import AzureOpenAI
from dotenv import load_dotenv
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select

from ..core.ffmpeg import feed_ffmpeg_async
from ..db.database import get_async_session
from ..db.models import TranscriptSegment, Transcript

load_dotenv()

logger = logging.getLogger(__name__)

# å„ªåŒ–å¾Œçš„æ‰¹æ¬¡è™•ç†è¨­å®š - ç›®æ¨™å»¶é² â‰¤3ç§’
BATCH_SIZE = int(os.getenv("WHISPER_BATCH_SIZE", "1"))  # æ”¹ç‚º1å€‹åˆ‡ç‰‡ç«‹å³è™•ç†
BATCH_TIMEOUT = int(os.getenv("WHISPER_BATCH_TIMEOUT", "2"))  # æ”¹ç‚º2ç§’è¶…æ™‚

# æ•ˆèƒ½ç›£æ§è¨­å®š
ENABLE_PERFORMANCE_LOGGING = os.getenv("ENABLE_PERFORMANCE_LOGGING", "true").lower() == "true"

class PerformanceTimer:
    """æ•ˆèƒ½è¨ˆæ™‚å™¨"""

    def __init__(self, operation_name: str):
        self.operation_name = operation_name
        self.start_time = None
        self.end_time = None

    def __enter__(self):
        self.start_time = time.time()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.end_time = time.time()
        duration = self.get_duration()

        if ENABLE_PERFORMANCE_LOGGING:
            if duration > 1.0:  # è¨˜éŒ„è¶…é1ç§’çš„æ“ä½œ
                logger.warning(f"âš ï¸  {self.operation_name} took {duration:.2f}s (slow)")
            else:
                logger.info(f"â±ï¸  {self.operation_name} completed in {duration:.2f}s")

    def get_duration(self) -> float:
        if self.start_time and self.end_time:
            return self.end_time - self.start_time
        return 0.0

class AudioBatch:
    """éŸ³è¨Šæ‰¹æ¬¡è™•ç†é¡åˆ¥"""

    def __init__(self, session_id: UUID):
        self.session_id = session_id
        self.chunks: List[Dict[str, Any]] = []
        self.created_at = datetime.utcnow()
        self.is_processing = False

    def add_chunk(self, chunk_sequence: int, audio_data: bytes, duration: float = 0.0):
        """æ·»åŠ éŸ³è¨Šåˆ‡ç‰‡åˆ°æ‰¹æ¬¡"""
        self.chunks.append({
            'sequence': chunk_sequence,
            'audio_data': audio_data,
            'duration': duration,
            'added_at': datetime.utcnow()
        })

    def is_ready_for_processing(self) -> bool:
        """æª¢æŸ¥æ‰¹æ¬¡æ˜¯å¦æº–å‚™å¥½è™•ç†"""
        if self.is_processing:
            return False

        # é”åˆ°æ‰¹æ¬¡å¤§å°æˆ–è¶…æ™‚
        chunk_count = len(self.chunks)
        elapsed_time = (datetime.utcnow() - self.created_at).total_seconds()

        is_ready = chunk_count >= BATCH_SIZE or elapsed_time >= BATCH_TIMEOUT

        if is_ready and ENABLE_PERFORMANCE_LOGGING:
            reason = f"chunks={chunk_count}" if chunk_count >= BATCH_SIZE else f"timeout={elapsed_time:.1f}s"
            logger.info(f"ğŸš€ Batch ready for processing: {reason}")

        return is_ready

    def get_total_duration(self) -> float:
        """ç²å–æ‰¹æ¬¡ç¸½æ™‚é•·"""
        return sum(chunk['duration'] for chunk in self.chunks)

    def get_batch_age(self) -> float:
        """ç²å–æ‰¹æ¬¡å­˜åœ¨æ™‚é–“ï¼ˆç§’ï¼‰"""
        return (datetime.utcnow() - self.created_at).total_seconds()

class AudioTranscriptionService:
    """éŸ³è¨Šè½‰éŒ„æœå‹™"""

    def __init__(self):
        self.client = get_azure_openai_client()
        self.deployment_name = get_whisper_deployment_name()
        self.batches: Dict[UUID, AudioBatch] = {}
        self.session_headers: Dict[UUID, bytes] = {}

        # ä½µç™¼æ§åˆ¶ï¼šæ¯å€‹ session ä¸€å€‹è™•ç†é–
        self.processing_locks: Dict[UUID, asyncio.Lock] = {}

        # æ•ˆèƒ½çµ±è¨ˆ
        self.performance_stats = {
            'total_batches_processed': 0,
            'total_processing_time': 0.0,
            'average_processing_time': 0.0,
            'max_processing_time': 0.0,
            'min_processing_time': float('inf')
        }

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–æœå‹™"""
        if self.client is None:
            logger.warning("Azure OpenAI client not available - transcription disabled")
            return False

        if self.deployment_name is None:
            logger.warning("Whisper deployment name not configured - transcription disabled")
            return False

        logger.info(f"ğŸ¤ Audio transcription service initialized - batch_size={BATCH_SIZE}, timeout={BATCH_TIMEOUT}s")

        # é è¼‰å…¥æ´»èº session çš„ header chunks
        await self._preload_active_session_headers()

        return True

    async def _preload_active_session_headers(self):
        """é è¼‰å…¥æ‰€æœ‰æ´»èº session çš„ header chunks"""
        try:
            from ..db.database import get_supabase_client

            supabase_client = get_supabase_client()

            # æŸ¥è©¢æ‰€æœ‰æ´»èºçš„éŒ„éŸ³ session
            response = supabase_client.table("sessions").select("id").eq("status", "active").eq("type", "recording").execute()

            active_sessions = response.data if response.data else []
            logger.info(f"Found {len(active_sessions)} active recording sessions")

            # ç‚ºæ¯å€‹æ´»èº session é è¼‰å…¥ header chunk
            loaded_count = 0
            for session_data in active_sessions:
                session_id = UUID(session_data['id'])

                # æª¢æŸ¥æ˜¯å¦å·²ç¶“æœ‰ header
                if session_id not in self.session_headers:
                    header_data = await self._recover_header_chunk(session_id)
                    if header_data:
                        self.session_headers[session_id] = header_data
                        loaded_count += 1
                        logger.debug(f"Preloaded header for session {session_id}")

            logger.info(f"âœ… Preloaded {loaded_count} header chunks for active sessions")

        except Exception as e:
            logger.warning(f"Failed to preload active session headers: {e}")
            # é è¼‰å…¥å¤±æ•—ä¸å½±éŸ¿æœå‹™æ­£å¸¸é‹è¡Œ

    async def add_audio_chunk(self, session_id: UUID, chunk_sequence: int, audio_data: bytes):
        """æ·»åŠ éŸ³è¨Šåˆ‡ç‰‡åˆ°æ‰¹æ¬¡è™•ç†ä½‡åˆ—"""
        if not self.client or not self.deployment_name:
            logger.warning("Transcription service not available")
            return

        with PerformanceTimer(f"Add chunk {chunk_sequence} to batch"):
            # å¦‚æœæ˜¯ç¬¬ä¸€å€‹åˆ‡ç‰‡ï¼Œå„²å­˜ç‚º header
            if chunk_sequence == 0:
                self.session_headers[session_id] = audio_data
                logger.info(f"Stored header chunk (seq 0) for session {session_id}, size={len(audio_data)} bytes.")

            # ç¢ºä¿ session æœ‰è™•ç†é–
            if session_id not in self.processing_locks:
                self.processing_locks[session_id] = asyncio.Lock()

            # ç²å–æˆ–å»ºç«‹æ‰¹æ¬¡
            if session_id not in self.batches:
                self.batches[session_id] = AudioBatch(session_id)

            batch = self.batches[session_id]

            # å¦‚æœæ‰¹æ¬¡æ­£åœ¨è™•ç†ï¼Œå»ºç«‹æ–°æ‰¹æ¬¡
            if batch.is_processing:
                new_batch = AudioBatch(session_id)
                self.batches[session_id] = new_batch
                batch = new_batch
                logger.debug(f"Created new batch for session {session_id} as previous batch is processing")

            # æ·»åŠ åˆ‡ç‰‡åˆ°æ‰¹æ¬¡
            batch.add_chunk(chunk_sequence, audio_data)
            logger.debug(f"Added chunk {chunk_sequence} to batch for session {session_id}")

            # æª¢æŸ¥æ˜¯å¦æº–å‚™è™•ç†ï¼ˆä½¿ç”¨é–ç¢ºä¿åªæœ‰ä¸€å€‹è™•ç†ä»»å‹™ï¼‰
            if batch.is_ready_for_processing() and not batch.is_processing:
                # ç«‹å³æ¨™è¨˜ç‚ºè™•ç†ä¸­ï¼Œé˜²æ­¢é‡è¤‡è§¸ç™¼
                batch.is_processing = True
                # å•Ÿå‹•èƒŒæ™¯è™•ç†ä»»å‹™
                asyncio.create_task(self._process_batch_safely(session_id, batch))
                logger.debug(f"Started background batch processing for session {session_id}")

    async def _process_batch_safely(self, session_id: UUID, batch: AudioBatch):
        """å®‰å…¨åœ°è™•ç†éŸ³è¨Šæ‰¹æ¬¡ï¼Œä½¿ç”¨é–æ©Ÿåˆ¶"""
        if session_id not in self.processing_locks:
            logger.error(f"No processing lock found for session {session_id}")
            batch.is_processing = False
            return

        async with self.processing_locks[session_id]:
            try:
                await self._process_batch(session_id, batch)
            except Exception as e:
                logger.error(f"Unexpected error in batch processing for session {session_id}: {e}", exc_info=True)
            finally:
                # ç¢ºä¿è™•ç†å®Œæˆå¾Œé‡ç½®ç‹€æ…‹
                batch.is_processing = False

    async def _process_batch(self, session_id: UUID, target_batch: AudioBatch):
        """è™•ç†éŸ³è¨Šæ‰¹æ¬¡"""
        batch_start_time = time.time()

        try:
            with PerformanceTimer(f"Process batch for session {session_id}"):
                logger.info(f"ğŸ”„ Processing batch for session {session_id} with {len(target_batch.chunks)} chunks (age: {target_batch.get_batch_age():.1f}s)")

                # åˆä½µéŸ³è¨Šåˆ‡ç‰‡
                with PerformanceTimer("Audio merging"):
                    merged_audio = await self._merge_audio_chunks(session_id, target_batch.chunks)
                    if not merged_audio:
                        logger.error(f"Failed to merge audio chunks for session {session_id}")
                        return

                # ç™¼é€åˆ° Azure OpenAI Whisper
                with PerformanceTimer("Azure OpenAI Whisper API"):
                    transcript_result = await self._transcribe_audio(merged_audio, session_id)
                    if transcript_result:
                        # å„²å­˜è½‰éŒ„çµæœåˆ°è³‡æ–™åº«ä¸¦æ¨é€åˆ° WebSocket
                        with PerformanceTimer("Save and push results"):
                            await self._save_and_push_transcript_result(session_id, transcript_result, target_batch.chunks)

                # æ›´æ–°æ•ˆèƒ½çµ±è¨ˆ
                batch_duration = time.time() - batch_start_time
                self._update_performance_stats(batch_duration, session_id)

                # æª¢æŸ¥æ˜¯å¦è¶…éç›®æ¨™å»¶é²
                if batch_duration > 5.0:
                    logger.warning(
                        f"âš ï¸ æ‰¹æ¬¡è™•ç†è¶…éç›®æ¨™å»¶é²: {batch_duration:.2f}s > 5.0s "
                        f"(session: {session_id}, chunks: {len(target_batch.chunks)})"
                    )

        except Exception as e:
            logger.error(f"Error processing batch for session {session_id}: {e}", exc_info=True)
        finally:
            # æ¸…ç†å·²è™•ç†çš„æ‰¹æ¬¡
            if session_id in self.batches and self.batches[session_id] == target_batch:
                # å‰µå»ºæ–°çš„ç©ºæ‰¹æ¬¡æ›¿æ›å·²è™•ç†çš„æ‰¹æ¬¡
                self.batches[session_id] = AudioBatch(session_id)
                logger.debug(f"Cleaned up processed batch for session {session_id}")

            logger.info(f"âœ… Batch processing completed for session {session_id}")

    def _update_performance_stats(self, duration: float, session_id: UUID = None):
        """æ›´æ–°æ•ˆèƒ½çµ±è¨ˆ"""
        self.performance_stats['total_batches_processed'] += 1
        self.performance_stats['total_processing_time'] += duration
        self.performance_stats['average_processing_time'] = (
            self.performance_stats['total_processing_time'] /
            self.performance_stats['total_batches_processed']
        )
        self.performance_stats['max_processing_time'] = max(
            self.performance_stats['max_processing_time'],
            duration
        )
        self.performance_stats['min_processing_time'] = min(
            self.performance_stats['min_processing_time'],
            duration
        )

        # åˆ†é¡è¨˜éŒ„æ•ˆèƒ½
        if duration <= 2.0:
            performance_level = "ğŸŸ¢ å„ªç§€"
        elif duration <= 5.0:
            performance_level = "ğŸŸ¡ æ­£å¸¸"
        elif duration <= 10.0:
            performance_level = "ğŸŸ  ç·©æ…¢"
        else:
            performance_level = "ğŸ”´ åš´é‡å»¶é²"

        if ENABLE_PERFORMANCE_LOGGING:
            logger.info(f"ğŸ“Š æ‰¹æ¬¡è™•ç†å®Œæˆ ({performance_level}): "
                       f"duration={duration:.2f}s, "
                       f"avg={self.performance_stats['average_processing_time']:.2f}s, "
                       f"max={self.performance_stats['max_processing_time']:.2f}s, "
                       f"total_batches={self.performance_stats['total_batches_processed']}")

            # æ¯ 10 å€‹æ‰¹æ¬¡è¼¸å‡ºä¸€æ¬¡è©³ç´°çµ±è¨ˆ
            if self.performance_stats['total_batches_processed'] % 10 == 0:
                logger.info(f"ğŸ“ˆ è½‰éŒ„æœå‹™æ•ˆèƒ½çµ±è¨ˆ (æœ€è¿‘ 10 æ‰¹æ¬¡): "
                           f"å¹³å‡å»¶é²={self.performance_stats['average_processing_time']:.2f}s, "
                           f"æœ€å¤§å»¶é²={self.performance_stats['max_processing_time']:.2f}s, "
                           f"æœ€å°å»¶é²={self.performance_stats['min_processing_time']:.2f}s")

    def get_performance_report(self) -> Dict[str, Any]:
        """ç²å–æ•ˆèƒ½å ±å‘Š"""
        total_batches = self.performance_stats['total_batches_processed']
        avg_time = self.performance_stats['average_processing_time']

        # è¨ˆç®—æ•ˆèƒ½ç­‰ç´š
        if avg_time <= 2.0:
            performance_grade = "A+ (å„ªç§€)"
        elif avg_time <= 5.0:
            performance_grade = "A (è‰¯å¥½)"
        elif avg_time <= 8.0:
            performance_grade = "B (æ™®é€š)"
        elif avg_time <= 12.0:
            performance_grade = "C (éœ€è¦æ”¹é€²)"
        else:
            performance_grade = "D (åš´é‡å•é¡Œ)"

        return {
            **self.performance_stats,
            'current_batch_count': len(self.batches),
            'batch_size_config': BATCH_SIZE,
            'batch_timeout_config': BATCH_TIMEOUT,
            'performance_logging_enabled': ENABLE_PERFORMANCE_LOGGING,
            'performance_grade': performance_grade,
            'target_latency': 5.0,
            'latency_compliance': avg_time <= 5.0 if total_batches > 0 else True,
            'recommendations': self._get_performance_recommendations()
        }

    def _get_performance_recommendations(self) -> List[str]:
        """æ ¹æ“šç›®å‰æ•ˆèƒ½æä¾›æ”¹é€²å»ºè­°"""
        recommendations = []
        avg_time = self.performance_stats['average_processing_time']
        max_time = self.performance_stats['max_processing_time']

        if avg_time > 5.0:
            recommendations.append("å¹³å‡è™•ç†æ™‚é–“è¶…éç›®æ¨™ï¼Œå»ºè­°æª¢æŸ¥ Azure OpenAI API é…ç½®")

        if max_time > 15.0:
            recommendations.append("æœ€å¤§è™•ç†æ™‚é–“éé•·ï¼Œå¯èƒ½å­˜åœ¨ç¶²è·¯æˆ– API å•é¡Œ")

        if len(self.batches) > 5:
            recommendations.append("åŒæ™‚è™•ç†çš„æ‰¹æ¬¡éå¤šï¼Œå»ºè­°æª¢æŸ¥ç³»çµ±è² è¼‰")

        if BATCH_SIZE > 3:
            recommendations.append(f"æ‰¹æ¬¡å¤§å° ({BATCH_SIZE}) è¼ƒå¤§ï¼Œè€ƒæ…®é™ä½ä»¥æ”¹å–„å»¶é²")

        if not recommendations:
            recommendations.append("æ•ˆèƒ½è¡¨ç¾è‰¯å¥½ï¼Œç„¡éœ€ç‰¹åˆ¥èª¿æ•´")

        return recommendations

    async def _merge_audio_chunks(self, session_id: UUID, chunks: List[Dict[str, Any]]) -> Optional[bytes]:
        """åˆä½µéŸ³è¨Šåˆ‡ç‰‡ç‚ºå–®ä¸€ PCM éŸ³è¨Š"""
        if not chunks:
            return None

        try:
            # æŒ‰åºè™Ÿæ’åºåˆ‡ç‰‡
            sorted_chunks = sorted(chunks, key=lambda x: x['sequence'])
            sequences_in_batch = {c['sequence'] for c in sorted_chunks}

            # é©—è­‰éŸ³è¨Šåˆ‡ç‰‡æ•¸æ“š
            valid_chunks = []
            total_audio_size = 0

            for chunk in sorted_chunks:
                audio_data = chunk['audio_data']
                if not audio_data or len(audio_data) < 10:  # è‡³å°‘éœ€è¦ä¸€äº›æ•¸æ“š
                    logger.warning(f"Skipping empty or too small chunk {chunk['sequence']}: {len(audio_data) if audio_data else 0} bytes")
                    continue

                # æª¢æŸ¥æ˜¯å¦ç‚ºæœ‰æ•ˆçš„ WebM æ•¸æ“šï¼ˆæª¢æŸ¥ EBML æ¨™é ­ï¼‰
                if chunk['sequence'] == 0:  # Header chunk å¿…é ˆæœ‰ EBML æ¨™é ­
                    if not audio_data.startswith(b'\x1a\x45\xdf\xa3'):
                        logger.warning(f"Header chunk {chunk['sequence']} missing EBML header")
                        continue

                valid_chunks.append(chunk)
                total_audio_size += len(audio_data)

            if not valid_chunks:
                logger.error(f"No valid audio chunks found for session {session_id}")
                return None

            logger.debug(f"Found {len(valid_chunks)} valid chunks out of {len(sorted_chunks)} total, total size: {total_audio_size} bytes")

            # æ­¥é©Ÿ 1: å»ºç«‹ä¸€å€‹äºŒé€²åˆ¶ blobï¼Œå¿…è¦æ™‚å‰ç½® header
            merged_webm = BytesIO()

            # å¦‚æœæ‰¹æ¬¡ä¸­æ²’æœ‰ header (chunk 0)ï¼Œå‰‡å¾å­˜å„²ä¸­å‰ç½® header
            if 0 not in sequences_in_batch:
                if session_id in self.session_headers:
                    header_data = self.session_headers[session_id]
                    if header_data and len(header_data) > 0:
                        merged_webm.write(header_data)
                        logger.debug(f"Prepended stored header for session {session_id} to batch.")
                    else:
                        logger.warning(f"Stored header for session {session_id} is empty")
                else:
                    # å˜—è©¦å¾ R2 æ¢å¾© header chunk
                    header_chunk = await self._recover_header_chunk(session_id)
                    if header_chunk and len(header_chunk) > 0:
                        self.session_headers[session_id] = header_chunk
                        merged_webm.write(header_chunk)
                        logger.info(f"Recovered and prepended header chunk for session {session_id}")
                    else:
                        # æ²’æœ‰ headerï¼Œç„¡æ³•è™•ç†æ­¤æ‰¹æ¬¡
                        logger.error(f"Cannot process batch for session {session_id}: Header chunk (seq 0) not found in store.")
                        return None

            # å¯«å…¥ç•¶å‰æ‰¹æ¬¡çš„æ‰€æœ‰æœ‰æ•ˆåˆ‡ç‰‡
            for chunk in valid_chunks:
                merged_webm.write(chunk['audio_data'])

            merged_webm_data = merged_webm.getvalue()
            merged_webm.close()

            if not merged_webm_data or len(merged_webm_data) < 50:  # è‡³å°‘éœ€è¦åŸºæœ¬çš„ WebM çµæ§‹
                logger.warning(f"Merged audio data too small: {len(merged_webm_data) if merged_webm_data else 0} bytes")
                return None

            logger.debug(f"Merged {len(valid_chunks)} webm chunks into {len(merged_webm_data)} bytes.")

            # æ­¥é©Ÿ 2: ä¸€æ¬¡æ€§å°‡åˆä½µå¾Œçš„ WebM è½‰æ›ç‚º PCM
            with PerformanceTimer(f"FFmpeg conversion for {len(valid_chunks)} chunks"):
                try:
                    pcm_data = await feed_ffmpeg_async(merged_webm_data)
                    if not pcm_data or len(pcm_data) < 100:  # PCM æ•¸æ“šæ‡‰è©²æœ‰ä¸€å®šå¤§å°
                        logger.error(f"FFmpeg conversion resulted in insufficient data: {len(pcm_data) if pcm_data else 0} bytes")
                        return None

                    logger.debug(f"Converted merged webm to {len(pcm_data)} bytes of PCM.")
                    return pcm_data

                except Exception as e:
                    # åœ¨é€™è£¡æ•ç²ä¸¦è¨˜éŒ„è©³ç´°çš„ FFmpeg è½‰æ›éŒ¯èª¤
                    logger.error(f"FFmpeg conversion failed for a batch of {len(valid_chunks)} chunks: {e}", exc_info=True)
                    # å˜—è©¦ä¿å­˜å•é¡ŒéŸ³æª”ç”¨æ–¼èª¿è©¦
                    try:
                        import tempfile
                        with tempfile.NamedTemporaryFile(suffix='.webm', delete=False) as f:
                            f.write(merged_webm_data)
                            logger.error(f"Problematic WebM saved to: {f.name}")
                    except:
                        pass
                    return None

        except Exception as e:
            logger.error(f"Error merging audio chunks: {e}", exc_info=True)
            return None

    async def _recover_header_chunk(self, session_id: UUID) -> Optional[bytes]:
        """å¾ R2 æ¢å¾© header chunk (åºè™Ÿ 0)"""
        try:
            from ..services.r2_client import get_r2_client
            from ..services.r2_client import generate_audio_key

            r2_client = get_r2_client()
            header_key = generate_audio_key(str(session_id), 0)

            logger.info(f"Attempting to recover header chunk from R2: {header_key}")

            # å¾ R2 ä¸‹è¼‰ header chunk
            download_result = await r2_client.download_file(header_key)

            if download_result['success']:
                header_data = download_result['data']
                logger.info(f"Successfully recovered header chunk for session {session_id}, size: {len(header_data)} bytes")
                return header_data
            else:
                logger.warning(f"Failed to recover header chunk: {download_result.get('error')}")
                return None

        except Exception as e:
            logger.error(f"Failed to recover header chunk for session {session_id}: {e}")
            return None

    async def _transcribe_audio(self, audio_data: bytes, session_id: UUID) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨ Azure OpenAI Whisper è½‰éŒ„éŸ³è¨Š"""
        try:
            # å»ºç«‹è‡¨æ™‚éŸ³è¨Šæª”æ¡ˆ
            audio_file = BytesIO(audio_data)
            audio_file.name = f"audio_{session_id}.wav"

            # å‘¼å« Azure OpenAI Whisper API
            response = await asyncio.to_thread(
                self.client.audio.transcriptions.create,
                model=self.deployment_name,
                file=audio_file,
                response_format="verbose_json",
                language="zh"  # é è¨­ä¸­æ–‡ï¼Œå¯æ ¹æ“šéœ€è¦èª¿æ•´
            )

            # è™•ç†å›æ‡‰
            if response and response.text:
                result = {
                    'text': response.text.strip(),
                    'language': getattr(response, 'language', 'zh'),
                    'duration': getattr(response, 'duration', 0.0),
                    'segments': getattr(response, 'segments', []),
                    'timestamp': datetime.utcnow().isoformat()
                }

                logger.info(f"Transcription successful for session {session_id}: {len(response.text)} characters")
                return result
            else:
                logger.warning(f"Empty transcription result for session {session_id}")
                return None

        except Exception as e:
            logger.error(f"Transcription failed for session {session_id}: {e}")
            return None
        finally:
            if 'audio_file' in locals():
                audio_file.close()

    async def _save_and_push_transcript_result(self, session_id: UUID, transcript_result: Dict[str, Any], chunks: List[Dict[str, Any]]):
        """å„²å­˜è½‰éŒ„çµæœåˆ°è³‡æ–™åº«ä¸¦æ¨é€åˆ° WebSocket"""
        try:
            # è¨ˆç®—æ™‚é–“ç¯„åœ
            chunk_sequences = [chunk['sequence'] for chunk in chunks]
            start_sequence = min(chunk_sequences)
            end_sequence = max(chunk_sequences)

            # å„²å­˜åˆ°è³‡æ–™åº«
            segment_id = await self._save_transcript_segment(
                session_id=session_id,
                chunk_sequence=start_sequence,
                text=transcript_result['text'],
                start_time=0.0,  # æš«æ™‚ä½¿ç”¨ç›¸å°æ™‚é–“ï¼Œå¾ŒçºŒå¯å„ªåŒ–
                end_time=transcript_result.get('duration', 5.0),
                confidence=1.0  # Azure OpenAI ä¸æä¾›ä¿¡å¿ƒåº¦
            )

            if segment_id:
                logger.info(f"Saved transcript segment {segment_id} for session {session_id}")

                # æ›´æ–°å®Œæ•´é€å­—ç¨¿
                await self._update_full_transcript(session_id)

            # æº–å‚™æ¨é€è³‡æ–™
            push_data = {
                'type': 'transcript_segment',
                'session_id': str(session_id),
                'segment_id': str(segment_id) if segment_id else None,
                'text': transcript_result['text'],
                'start_sequence': start_sequence,
                'end_sequence': end_sequence,
                'start_time': 0.0,
                'end_time': transcript_result.get('duration', 5.0),
                'language': transcript_result.get('language', 'zh'),
                'confidence': 1.0,
                'timestamp': transcript_result['timestamp']
            }

            # æ¨é€åˆ° WebSocket
            from ..ws.transcript_feed import manager
            await manager.broadcast_to_session(session_id, push_data)

            logger.info(f"Pushed transcript result to session {session_id}: {transcript_result['text'][:50]}...")

        except Exception as e:
            logger.error(f"Failed to save and push transcript result for session {session_id}: {e}")

    async def _save_transcript_segment(self, session_id: UUID, chunk_sequence: int, text: str,
                                     start_time: float, end_time: float, confidence: float) -> Optional[UUID]:
        """å„²å­˜é€å­—ç¨¿ç‰‡æ®µåˆ°è³‡æ–™åº«"""
        try:
            async for db in get_async_session():
                # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„ç‰‡æ®µ
                existing_segment = await db.execute(
                    select(TranscriptSegment).where(
                        TranscriptSegment.session_id == session_id,
                        TranscriptSegment.chunk_sequence == chunk_sequence
                    )
                )
                existing = existing_segment.scalar_one_or_none()

                if existing:
                    # æ›´æ–°ç¾æœ‰ç‰‡æ®µ
                    existing.text = text
                    existing.start_time = Decimal(str(start_time))
                    existing.end_time = Decimal(str(end_time))
                    existing.confidence = Decimal(str(confidence))
                    segment_id = existing.id
                    logger.debug(f"Updated existing transcript segment {segment_id}")
                else:
                    # å»ºç«‹æ–°ç‰‡æ®µ
                    new_segment = TranscriptSegment(
                        session_id=session_id,
                        chunk_sequence=chunk_sequence,
                        start_time=Decimal(str(start_time)),
                        end_time=Decimal(str(end_time)),
                        text=text,
                        confidence=Decimal(str(confidence))
                    )
                    db.add(new_segment)
                    await db.flush()  # ç²å– ID
                    segment_id = new_segment.id
                    logger.debug(f"Created new transcript segment {segment_id}")

                await db.commit()
                return segment_id

        except Exception as e:
            logger.error(f"Failed to save transcript segment: {e}")
            return None

    async def _update_full_transcript(self, session_id: UUID):
        """æ›´æ–°å®Œæ•´é€å­—ç¨¿"""
        try:
            async for db in get_async_session():
                # ç²å–æ‰€æœ‰ç‰‡æ®µï¼ŒæŒ‰åºè™Ÿæ’åº
                segments_result = await db.execute(
                    select(TranscriptSegment)
                    .where(TranscriptSegment.session_id == session_id)
                    .order_by(TranscriptSegment.chunk_sequence)
                )
                segments = segments_result.scalars().all()

                if not segments:
                    return

                # åˆä½µæ‰€æœ‰æ–‡å­—
                full_text = " ".join(segment.text for segment in segments)

                # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨å®Œæ•´é€å­—ç¨¿
                existing_transcript = await db.execute(
                    select(Transcript).where(Transcript.session_id == session_id)
                )
                existing = existing_transcript.scalar_one_or_none()

                if existing:
                    # æ›´æ–°ç¾æœ‰é€å­—ç¨¿
                    existing.full_text = full_text
                    logger.debug(f"Updated full transcript for session {session_id}")
                else:
                    # å»ºç«‹æ–°é€å­—ç¨¿
                    new_transcript = Transcript(
                        session_id=session_id,
                        full_text=full_text
                    )
                    db.add(new_transcript)
                    logger.debug(f"Created full transcript for session {session_id}")

                await db.commit()

        except Exception as e:
            logger.error(f"Failed to update full transcript for session {session_id}: {e}")

    async def cleanup(self):
        """æ¸…ç†æœå‹™è³‡æº"""
        logger.info("Cleaning up AudioTranscriptionService...")

        # ç­‰å¾…æ‰€æœ‰æ­£åœ¨è™•ç†çš„æ‰¹æ¬¡å®Œæˆ
        for session_id, lock in self.processing_locks.items():
            try:
                # å˜—è©¦ç²å–é–ï¼Œç­‰å¾…æ­£åœ¨é€²è¡Œçš„è™•ç†å®Œæˆ
                async with asyncio.timeout(10):  # æœ€å¤šç­‰å¾… 10 ç§’
                    async with lock:
                        pass  # é–ç²å–æˆåŠŸï¼Œè¡¨ç¤ºè™•ç†å·²å®Œæˆ
                logger.debug(f"Processing completed for session {session_id}")
            except asyncio.TimeoutError:
                logger.warning(f"Timeout waiting for batch processing to complete for session {session_id}")
            except Exception as e:
                logger.warning(f"Error during cleanup for session {session_id}: {e}")

        # æ¸…ç†æ‰¹æ¬¡ï¼Œä½†ä¿ç•™ header chunks
        for session_id in list(self.batches.keys()):
            if session_id in self.batches:
                batch = self.batches[session_id]
                if not batch.is_processing:
                    del self.batches[session_id]
                    logger.debug(f"Cleaned up batch for session {session_id}")

        # æ¸…ç†è™•ç†é–
        self.processing_locks.clear()

        # å¯ä»¥é¸æ“‡ä¿ç•™ header chunks æˆ–æ¸…ç†å®ƒå€‘
        # é€™è£¡æˆ‘å€‘ä¿ç•™ header chunks ä»¥ä¾¿å¾ŒçºŒä½¿ç”¨
        logger.info(f"Cleanup completed. Retained {len(self.session_headers)} header chunks.")

    def diagnose_session(self, session_id: UUID) -> Dict[str, Any]:
        """è¨ºæ–·ç‰¹å®š session çš„ç‹€æ…‹"""
        diagnosis = {
            'session_id': str(session_id),
            'has_header': session_id in self.session_headers,
            'header_size': len(self.session_headers.get(session_id, b'')),
            'has_batch': session_id in self.batches,
            'batch_info': None,
            'performance_stats': self.performance_stats.copy()
        }

        if session_id in self.batches:
            batch = self.batches[session_id]
            diagnosis['batch_info'] = {
                'chunk_count': len(batch.chunks),
                'is_processing': batch.is_processing,
                'batch_age_seconds': batch.get_batch_age(),
                'total_duration': batch.get_total_duration(),
                'chunk_sequences': [chunk['sequence'] for chunk in batch.chunks]
            }

        return diagnosis

    def get_all_session_status(self) -> Dict[str, Any]:
        """ç²å–æ‰€æœ‰ session çš„ç‹€æ…‹æ¦‚è¦½"""
        return {
            'total_sessions_with_headers': len(self.session_headers),
            'total_active_batches': len(self.batches),
            'sessions_with_headers': list(str(sid) for sid in self.session_headers.keys()),
            'sessions_with_batches': list(str(sid) for sid in self.batches.keys()),
            'performance_stats': self.performance_stats.copy()
        }

def get_azure_openai_client() -> AzureOpenAI | None:
    """
    Initializes and returns the Azure OpenAI client.
    Returns None if the required environment variables are not set.
    """
    try:
        api_key = os.environ["AZURE_OPENAI_API_KEY"]
        azure_endpoint = os.environ["AZURE_OPENAI_ENDPOINT"]
        api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-01")

        return AzureOpenAI(
            api_key=api_key,
            api_version=api_version,
            azure_endpoint=azure_endpoint,
        )
    except KeyError as e:
        logger.warning(f"Azure OpenAI environment variable not set: {e}. The service will be disabled.")
        return None


def get_whisper_deployment_name() -> str | None:
    """
    Returns the Whisper deployment name from environment variables.
    """
    try:
        deployment_name = os.getenv("WHISPER_DEPLOYMENT_NAME", "whisper-1")
        if not deployment_name:
            raise ValueError("WHISPER_DEPLOYMENT_NAME is set to an empty string.")
        return deployment_name
    except ValueError as e:
        logger.warning(f"{e} The service will be disabled.")
        return None

# å…¨åŸŸè½‰éŒ„æœå‹™å¯¦ä¾‹
_transcription_service: Optional[AudioTranscriptionService] = None

async def get_transcription_service() -> Optional[AudioTranscriptionService]:
    """ç²å–è½‰éŒ„æœå‹™å¯¦ä¾‹"""
    global _transcription_service

    if _transcription_service is None:
        _transcription_service = AudioTranscriptionService()
        if not await _transcription_service.initialize():
            _transcription_service = None

    return _transcription_service
