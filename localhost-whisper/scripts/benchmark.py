#!/usr/bin/env python3
"""
MLX Whisper API æ•ˆèƒ½æ¸¬è©¦å·¥å…·

æ¸¬è©¦ API çš„å„é …æ•ˆèƒ½æŒ‡æ¨™ï¼ŒåŒ…æ‹¬ï¼š
- è«‹æ±‚è™•ç†æ™‚é–“
- æ¨¡å‹è¼‰å…¥æ™‚é–“
- è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³
- ä½µç™¼è™•ç†èƒ½åŠ›
- ä¸åŒæ¨¡å‹çš„æ•ˆèƒ½å°æ¯”
"""

import argparse
import asyncio
import json
import logging
import statistics
import sys
import time
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import List, Dict, Any, Optional

import aiohttp
import psutil
import requests
from requests_toolbelt.multipart.encoder import MultipartEncoder

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

logger = logging.getLogger(__name__)


class BenchmarkResult:
    """æ•ˆèƒ½æ¸¬è©¦çµæœ"""

    def __init__(self):
        self.test_name = ""
        self.total_requests = 0
        self.successful_requests = 0
        self.failed_requests = 0
        self.total_time = 0.0
        self.response_times = []
        self.errors = []
        self.model_name = ""
        self.file_size = 0
        self.audio_duration = 0.0

    def add_result(self, response_time: float, success: bool, error: str = None):
        """æ·»åŠ å–®æ¬¡æ¸¬è©¦çµæœ"""
        self.total_requests += 1
        self.response_times.append(response_time)

        if success:
            self.successful_requests += 1
        else:
            self.failed_requests += 1
            if error:
                self.errors.append(error)

    def get_statistics(self) -> Dict[str, Any]:
        """è¨ˆç®—çµ±è¨ˆè³‡æ–™"""
        if not self.response_times:
            return {}

        return {
            "test_name": self.test_name,
            "model": self.model_name,
            "total_requests": self.total_requests,
            "successful_requests": self.successful_requests,
            "failed_requests": self.failed_requests,
            "success_rate": (self.successful_requests / self.total_requests) * 100,
            "total_time": self.total_time,
            "avg_response_time": statistics.mean(self.response_times),
            "min_response_time": min(self.response_times),
            "max_response_time": max(self.response_times),
            "median_response_time": statistics.median(self.response_times),
            "std_response_time": (
                statistics.stdev(self.response_times)
                if len(self.response_times) > 1
                else 0
            ),
            "requests_per_second": (
                self.successful_requests / self.total_time if self.total_time > 0 else 0
            ),
            "audio_duration": self.audio_duration,
            "file_size_mb": self.file_size / (1024 * 1024) if self.file_size > 0 else 0,
            "processing_speed_ratio": (
                self.audio_duration / statistics.mean(self.response_times)
                if self.response_times and self.audio_duration > 0
                else 0
            ),
            "errors": self.errors[:5],  # åªä¿ç•™å‰5å€‹éŒ¯èª¤
        }


class APIBenchmark:
    """API æ•ˆèƒ½æ¸¬è©¦å™¨"""

    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url.rstrip("/")
        self.session = requests.Session()

    async def check_health(self) -> bool:
        """æª¢æŸ¥ API å¥åº·ç‹€æ…‹"""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=10)
            return response.status_code == 200
        except Exception as e:
            logger.error(f"å¥åº·æª¢æŸ¥å¤±æ•—: {str(e)}")
            return False

    def get_system_info(self) -> Dict[str, Any]:
        """å–å¾—ç³»çµ±è³‡è¨Š"""
        return {
            "cpu_count": psutil.cpu_count(),
            "memory_total": psutil.virtual_memory().total,
            "memory_available": psutil.virtual_memory().available,
            "memory_percent": psutil.virtual_memory().percent,
        }

    def create_test_audio(self, duration: int = 10) -> bytes:
        """å‰µå»ºæ¸¬è©¦éŸ³é »æª”æ¡ˆï¼ˆç°¡å–®çš„éœéŸ³ WAVï¼‰"""
        import wave
        import struct
        import io

        # å‰µå»ºç°¡å–®çš„ WAV æª”æ¡ˆ
        sample_rate = 16000
        num_samples = sample_rate * duration

        # å‰µå»ºéœéŸ³
        audio_data = [0] * num_samples

        # å¯«å…¥ WAV æª”æ¡ˆ
        buffer = io.BytesIO()
        with wave.open(buffer, "wb") as wav_file:
            wav_file.setnchannels(1)  # å–®è²é“
            wav_file.setsampwidth(2)  # 16 bit
            wav_file.setframerate(sample_rate)
            wav_file.writeframes(struct.pack("<" + "h" * len(audio_data), *audio_data))

        buffer.seek(0)
        return buffer.read()

    def single_transcription_test(
        self,
        audio_data: bytes,
        model: str = "whisper-base",
        language: str = None,
        response_format: str = "json",
    ) -> tuple:
        """å–®æ¬¡è½‰éŒ„æ¸¬è©¦"""
        start_time = time.time()

        try:
            # æº–å‚™æª”æ¡ˆ
            files = {
                "file": ("test_audio.wav", audio_data, "audio/wav"),
                "model": (None, model),
                "response_format": (None, response_format),
            }

            if language:
                files["language"] = (None, language)

            # ç™¼é€è«‹æ±‚
            response = self.session.post(
                f"{self.base_url}/v1/audio/transcriptions",
                files=files,
                timeout=300,  # 5åˆ†é˜è¶…æ™‚
            )

            response_time = time.time() - start_time

            if response.status_code == 200:
                return (
                    response_time,
                    True,
                    response.json() if response_format == "json" else response.text,
                )
            else:
                return (
                    response_time,
                    False,
                    f"HTTP {response.status_code}: {response.text}",
                )

        except Exception as e:
            response_time = time.time() - start_time
            return response_time, False, str(e)

    def concurrent_test(
        self,
        audio_data: bytes,
        model: str = "whisper-base",
        num_requests: int = 10,
        max_workers: int = 4,
    ) -> BenchmarkResult:
        """ä½µç™¼æ¸¬è©¦"""
        logger.info(f"é–‹å§‹ä½µç™¼æ¸¬è©¦: {num_requests} å€‹è«‹æ±‚ï¼Œæœ€å¤§ {max_workers} å€‹ä½µç™¼")

        result = BenchmarkResult()
        result.test_name = "ä½µç™¼è™•ç†æ¸¬è©¦"
        result.model_name = model
        result.file_size = len(audio_data)

        start_time = time.time()

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = []

            for i in range(num_requests):
                future = executor.submit(
                    self.single_transcription_test, audio_data, model, None, "json"
                )
                futures.append(future)

            # æ”¶é›†çµæœ
            for i, future in enumerate(futures):
                try:
                    response_time, success, response_data = future.result()
                    result.add_result(
                        response_time, success, None if success else response_data
                    )

                    if i % 5 == 0:
                        logger.info(f"å®Œæˆ {i+1}/{num_requests} å€‹è«‹æ±‚")

                except Exception as e:
                    result.add_result(0, False, str(e))

        result.total_time = time.time() - start_time
        return result

    def model_comparison_test(
        self, audio_data: bytes, models: List[str], num_requests: int = 3
    ) -> Dict[str, BenchmarkResult]:
        """æ¨¡å‹æ•ˆèƒ½å°æ¯”æ¸¬è©¦"""
        logger.info(f"é–‹å§‹æ¨¡å‹å°æ¯”æ¸¬è©¦: {models}")

        results = {}

        for model in models:
            logger.info(f"æ¸¬è©¦æ¨¡å‹: {model}")

            result = BenchmarkResult()
            result.test_name = "æ¨¡å‹æ•ˆèƒ½å°æ¯”"
            result.model_name = model
            result.file_size = len(audio_data)

            start_time = time.time()

            for i in range(num_requests):
                response_time, success, response_data = self.single_transcription_test(
                    audio_data, model, None, "json"
                )
                result.add_result(
                    response_time, success, None if success else response_data
                )

                logger.info(
                    f"  {model} - è«‹æ±‚ {i+1}/{num_requests}: {response_time:.2f}s"
                )

            result.total_time = time.time() - start_time
            results[model] = result

        return results

    def memory_stress_test(
        self,
        large_audio_data: bytes,
        model: str = "whisper-base",
        num_requests: int = 5,
    ) -> BenchmarkResult:
        """è¨˜æ†¶é«”å£“åŠ›æ¸¬è©¦"""
        logger.info("é–‹å§‹è¨˜æ†¶é«”å£“åŠ›æ¸¬è©¦")

        result = BenchmarkResult()
        result.test_name = "è¨˜æ†¶é«”å£“åŠ›æ¸¬è©¦"
        result.model_name = model
        result.file_size = len(large_audio_data)

        # è¨˜éŒ„åˆå§‹è¨˜æ†¶é«”ä½¿ç”¨
        initial_memory = psutil.virtual_memory().percent
        logger.info(f"åˆå§‹è¨˜æ†¶é«”ä½¿ç”¨: {initial_memory:.1f}%")

        start_time = time.time()

        for i in range(num_requests):
            # ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨
            current_memory = psutil.virtual_memory().percent
            logger.info(f"  è«‹æ±‚ {i+1} é–‹å§‹ - è¨˜æ†¶é«”ä½¿ç”¨: {current_memory:.1f}%")

            response_time, success, response_data = self.single_transcription_test(
                large_audio_data, model, None, "json"
            )
            result.add_result(
                response_time, success, None if success else response_data
            )

            # ç­‰å¾…è¨˜æ†¶é«”å›æ”¶
            time.sleep(2)

        result.total_time = time.time() - start_time

        final_memory = psutil.virtual_memory().percent
        logger.info(
            f"æœ€çµ‚è¨˜æ†¶é«”ä½¿ç”¨: {final_memory:.1f}% (å¢åŠ : {final_memory - initial_memory:.1f}%)"
        )

        return result


def print_results(results: Dict[str, Any]):
    """ç¾åŒ–è¼¸å‡ºæ¸¬è©¦çµæœ"""
    print("\n" + "=" * 80)
    print("ğŸ¯ MLX Whisper API æ•ˆèƒ½æ¸¬è©¦å ±å‘Š")
    print("=" * 80)

    # ç³»çµ±è³‡è¨Š
    if "system_info" in results:
        sys_info = results["system_info"]
        print(f"\nğŸ’» ç³»çµ±è³‡è¨Š:")
        print(f"   CPU æ ¸å¿ƒæ•¸: {sys_info.get('cpu_count', 'N/A')}")
        print(f"   ç¸½è¨˜æ†¶é«”: {sys_info.get('memory_total', 0) / (1024**3):.1f} GB")
        print(
            f"   å¯ç”¨è¨˜æ†¶é«”: {sys_info.get('memory_available', 0) / (1024**3):.1f} GB"
        )
        print(f"   è¨˜æ†¶é«”ä½¿ç”¨ç‡: {sys_info.get('memory_percent', 0):.1f}%")

    # æ¸¬è©¦çµæœ
    for test_name, result in results.items():
        if test_name == "system_info":
            continue

        if isinstance(result, dict) and "test_name" in result:
            print(f"\nğŸ“Š {result['test_name']} ({result.get('model', 'N/A')})")
            print("-" * 60)
            print(f"   ç¸½è«‹æ±‚æ•¸: {result['total_requests']}")
            print(f"   æˆåŠŸè«‹æ±‚: {result['successful_requests']}")
            print(f"   å¤±æ•—è«‹æ±‚: {result['failed_requests']}")
            print(f"   æˆåŠŸç‡: {result['success_rate']:.1f}%")
            print(f"   ç¸½è€—æ™‚: {result['total_time']:.2f}s")
            print(f"   å¹³å‡å›æ‡‰æ™‚é–“: {result['avg_response_time']:.2f}s")
            print(f"   æœ€å°å›æ‡‰æ™‚é–“: {result['min_response_time']:.2f}s")
            print(f"   æœ€å¤§å›æ‡‰æ™‚é–“: {result['max_response_time']:.2f}s")
            print(f"   æ¯ç§’è«‹æ±‚æ•¸: {result['requests_per_second']:.2f}")

            if result.get("processing_speed_ratio", 0) > 0:
                print(f"   è™•ç†é€Ÿåº¦æ¯”ç‡: {result['processing_speed_ratio']:.2f}x")

            if result.get("errors"):
                print(f"   éŒ¯èª¤ç¯„ä¾‹: {result['errors'][0]}")

        elif isinstance(result, dict):  # æ¨¡å‹å°æ¯”çµæœ
            print(f"\nğŸ“Š {test_name}")
            print("-" * 60)

            for model_name, model_result in result.items():
                stats = (
                    model_result.get_statistics()
                    if hasattr(model_result, "get_statistics")
                    else model_result
                )
                print(f"   {model_name}:")
                print(f"     å¹³å‡å›æ‡‰æ™‚é–“: {stats['avg_response_time']:.2f}s")
                print(f"     æˆåŠŸç‡: {stats['success_rate']:.1f}%")
                print(f"     æ¯ç§’è«‹æ±‚æ•¸: {stats['requests_per_second']:.2f}")


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="MLX Whisper API æ•ˆèƒ½æ¸¬è©¦")

    parser.add_argument(
        "--base-url", default="http://localhost:8000", help="API åŸºç¤ URL"
    )

    parser.add_argument(
        "--test-type",
        choices=["single", "concurrent", "models", "memory", "all"],
        default="all",
        help="æ¸¬è©¦é¡å‹",
    )

    parser.add_argument("--model", default="whisper-base", help="æ¸¬è©¦æ¨¡å‹")

    parser.add_argument("--num-requests", type=int, default=10, help="è«‹æ±‚æ•¸é‡")

    parser.add_argument("--max-workers", type=int, default=4, help="æœ€å¤§ä½µç™¼æ•¸")

    parser.add_argument(
        "--audio-duration", type=int, default=10, help="æ¸¬è©¦éŸ³é »é•·åº¦ï¼ˆç§’ï¼‰"
    )

    parser.add_argument("--output", type=Path, help="çµæœè¼¸å‡ºæª”æ¡ˆ (JSON æ ¼å¼)")

    args = parser.parse_args()

    # åˆå§‹åŒ–æ¸¬è©¦å™¨
    benchmark = APIBenchmark(args.base_url)

    # æª¢æŸ¥ API å¥åº·ç‹€æ…‹
    logger.info("æª¢æŸ¥ API å¥åº·ç‹€æ…‹...")
    if not asyncio.run(benchmark.check_health()):
        logger.error("âŒ API æœå‹™ä¸å¯ç”¨ï¼Œè«‹å…ˆå•Ÿå‹•æœå‹™")
        sys.exit(1)

    logger.info("âœ… API æœå‹™æ­£å¸¸")

    # æº–å‚™æ¸¬è©¦éŸ³é »
    logger.info(f"æº–å‚™æ¸¬è©¦éŸ³é » ({args.audio_duration}ç§’)...")
    audio_data = benchmark.create_test_audio(args.audio_duration)
    large_audio_data = benchmark.create_test_audio(60)  # 1åˆ†é˜éŸ³é »ç”¨æ–¼å£“åŠ›æ¸¬è©¦

    # æ”¶é›†çµæœ
    all_results = {"system_info": benchmark.get_system_info(), "timestamp": time.time()}

    # åŸ·è¡Œæ¸¬è©¦
    if args.test_type in ["single", "all"]:
        logger.info("åŸ·è¡Œå–®æ¬¡è«‹æ±‚æ¸¬è©¦...")
        response_time, success, _ = benchmark.single_transcription_test(
            audio_data, args.model
        )

        result = BenchmarkResult()
        result.test_name = "å–®æ¬¡è«‹æ±‚æ¸¬è©¦"
        result.model_name = args.model
        result.add_result(response_time, success)
        result.total_time = response_time

        all_results["single_request"] = result.get_statistics()

    if args.test_type in ["concurrent", "all"]:
        logger.info("åŸ·è¡Œä½µç™¼æ¸¬è©¦...")
        result = benchmark.concurrent_test(
            audio_data, args.model, args.num_requests, args.max_workers
        )
        all_results["concurrent"] = result.get_statistics()

    if args.test_type in ["models", "all"]:
        logger.info("åŸ·è¡Œæ¨¡å‹å°æ¯”æ¸¬è©¦...")
        models = ["whisper-tiny", "whisper-base", "whisper-small"]
        results = benchmark.model_comparison_test(audio_data, models, 3)
        all_results["model_comparison"] = {
            model: result.get_statistics() for model, result in results.items()
        }

    if args.test_type in ["memory", "all"]:
        logger.info("åŸ·è¡Œè¨˜æ†¶é«”å£“åŠ›æ¸¬è©¦...")
        result = benchmark.memory_stress_test(large_audio_data, args.model, 3)
        all_results["memory_stress"] = result.get_statistics()

    # è¼¸å‡ºçµæœ
    print_results(all_results)

    # å„²å­˜çµæœæª”æ¡ˆ
    if args.output:
        with open(args.output, "w", encoding="utf-8") as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ“„ çµæœå·²å„²å­˜è‡³: {args.output}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
        sys.exit(130)
    except Exception as e:
        logger.error(f"âŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}", exc_info=True)
        sys.exit(1)
