"""
MLX Whisper API ä¸»æ‡‰ç”¨

æ•´åˆæ‰€æœ‰çµ„ä»¶ï¼Œæä¾›å®Œæ•´çš„ Whisper API æœå‹™ï¼Œ
ç¬¦åˆ OpenAI API è¦æ ¼ã€‚
"""

import asyncio
import logging
import time
from contextlib import asynccontextmanager
from typing import Dict, Any

from fastapi import FastAPI, Request, HTTPException, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from fastapi.responses import JSONResponse
from fastapi.openapi.docs import get_swagger_ui_html
from fastapi.openapi.utils import get_openapi

from app.config import get_settings
from app.api import health, models, transcriptions
from app.middleware.error_handler import ErrorHandlerMiddleware
from app.middleware.metrics import (
    PrometheusMetricsMiddleware,
    setup_metrics_integration,
)
from app.services.model_manager import get_model_manager
from app.services.rate_limiter import get_rate_limiter
from app.services.transcription import get_transcription_service

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("mlx_whisper_api.log", encoding="utf-8"),
    ],
)

logger = logging.getLogger(__name__)

# å–å¾—è¨­å®š
settings = get_settings()

# å•Ÿå‹•æ™‚åˆå§‹åŒ–æœå‹™
_startup_complete = False


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    æ‡‰ç”¨ç”Ÿå‘½é€±æœŸç®¡ç†å™¨

    è™•ç†æ‡‰ç”¨å•Ÿå‹•å’Œé—œé–‰æ™‚çš„åˆå§‹åŒ–å’Œæ¸…ç†å·¥ä½œ
    """
    global _startup_complete

    # å•Ÿå‹•éšæ®µ
    logger.info("ğŸš€ MLX Whisper API å•Ÿå‹•ä¸­...")

    try:
        # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        logger.info("ğŸ§  åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨...")
        model_manager = get_model_manager()

        # è·³éé è¼‰å…¥æ¨¡å‹ï¼Œé¿å…ç”¨æˆ¶äº¤äº’é˜»å¡æœå‹™å•Ÿå‹•
        logger.info("âš¡ è·³éé è¼‰å…¥æ¨¡å‹ï¼Œå°‡åœ¨é¦–æ¬¡è«‹æ±‚æ™‚å‹•æ…‹è¼‰å…¥")
        # å¦‚æœéœ€è¦é è¼‰å…¥ï¼Œå¯ä»¥åœ¨æœå‹™å•Ÿå‹•å¾Œç•°æ­¥åŸ·è¡Œ

        # åˆå§‹åŒ–é€Ÿç‡é™åˆ¶å™¨
        logger.info("ğŸ›¡ï¸ åˆå§‹åŒ–é€Ÿç‡é™åˆ¶å™¨...")
        rate_limiter = get_rate_limiter()

        # åˆå§‹åŒ–è½‰éŒ„æœå‹™
        logger.info("ğŸ¤ åˆå§‹åŒ–è½‰éŒ„æœå‹™...")
        transcription_service = get_transcription_service()

        # æœå‹™å¥åº·æª¢æŸ¥
        try:
            service_status = await transcription_service.get_service_status()
            if service_status.get("available", False):
                logger.info("âœ… è½‰éŒ„æœå‹™åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.warning("âš ï¸ è½‰éŒ„æœå‹™å¯èƒ½æœªå®Œå…¨å°±ç·’")
        except Exception as e:
            logger.warning(f"âš ï¸ è½‰éŒ„æœå‹™å¥åº·æª¢æŸ¥å¤±æ•—: {str(e)}")

        _startup_complete = True

        logger.info("ğŸ‰ MLX Whisper API å•Ÿå‹•å®Œæˆ!")
        logger.info(f"ğŸŒ æœå‹™é‹è¡Œåœ¨: http://{settings.host}:{settings.port}")
        logger.info(f"ğŸ“š API æ–‡ä»¶: http://{settings.host}:{settings.port}/docs")
        logger.info(f"ğŸ”§ Workers: {settings.workers}")
        logger.info(f"ğŸ“ æ—¥èªŒç­‰ç´š: {settings.log_level}")

        if settings.enable_metrics:
            logger.info(
                f"ğŸ“Š Prometheus æŒ‡æ¨™: http://{settings.host}:{settings.metrics_port}/metrics"
            )

        yield

    except Exception as e:
        logger.error(f"âŒ å•Ÿå‹•éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}", exc_info=True)
        _startup_complete = False
        raise

    # é—œé–‰éšæ®µ
    logger.info("ğŸ›‘ MLX Whisper API é—œé–‰ä¸­...")

    try:
        # æ¸…ç†æ¨¡å‹ç®¡ç†å™¨
        logger.info("ğŸ§  æ¸…ç†æ¨¡å‹ç®¡ç†å™¨...")
        await model_manager.shutdown()

        # æ¸…ç†è½‰éŒ„æœå‹™
        logger.info("ğŸ¤ æ¸…ç†è½‰éŒ„æœå‹™...")
        await transcription_service.shutdown()

        logger.info("âœ… MLX Whisper API å·²å®‰å…¨é—œé–‰")

    except Exception as e:
        logger.error(f"âŒ é—œé–‰éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}", exc_info=True)


# å‰µå»º FastAPI æ‡‰ç”¨
app = FastAPI(
    title="MLX Whisper API",
    description="""

    ## ğŸ“– ä½¿ç”¨æ–¹æ³•
    ä½¿ç”¨æ¨™æº–çš„ OpenAI Python SDK:
    ```python
    import openai

    client = openai.OpenAI(
        base_url="http://localhost:8000/v1",
        api_key="not-required"
    )

    with open("audio.mp3", "rb") as audio_file:
        transcript = client.audio.transcriptions.create(
            model="whisper-large-v3",
            file=audio_file
        )
    ```
    """,
    version="1.0.0",
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json",
)

# æ·»åŠ ä¸­é–“ä»¶

# 1. éŒ¯èª¤è™•ç†ä¸­é–“ä»¶ï¼ˆæœ€å¤–å±¤ï¼‰
app.add_middleware(ErrorHandlerMiddleware)

# 2. æŒ‡æ¨™ä¸­é–“ä»¶
if settings.enable_metrics:
    logger.info("ğŸ“Š å•Ÿç”¨æŒ‡æ¨™ä¸­é–“ä»¶")
    from app.middleware.metrics import (
        PrometheusMetricsMiddleware,
        setup_metrics_endpoint,
    )

    # ç›´æ¥æ·»åŠ æŒ‡æ¨™ä¸­é–“ä»¶
    app.add_middleware(PrometheusMetricsMiddleware)

    # è¨­å®šæŒ‡æ¨™ç«¯é»
    setup_metrics_endpoint(app)

# 3. CORS ä¸­é–“ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=(
        ["*"]
        if settings.debug
        else ["http://localhost:3000", "http://localhost:8000", "http://127.0.0.1:8000"]
    ),
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# 4. ä¿¡ä»»ä¸»æ©Ÿä¸­é–“ä»¶
# åœ¨é–‹ç™¼ç’°å¢ƒä¸­ä½¿ç”¨æ›´å¯¬é¬†çš„é…ç½®
if settings.debug:
    # é–‹ç™¼ç’°å¢ƒï¼šå…è¨±æ‰€æœ‰ä¸»æ©Ÿ
    app.add_middleware(
        TrustedHostMiddleware,
        allowed_hosts=["*"],
    )
elif settings.is_production():
    # ç”Ÿç”¢ç’°å¢ƒï¼šä½¿ç”¨åš´æ ¼çš„ä¸»æ©Ÿé™åˆ¶
    app.add_middleware(
        TrustedHostMiddleware,
        allowed_hosts=settings.get_allowed_hosts(),
    )

# åŒ…å«è·¯ç”±å™¨
app.include_router(health.router)
app.include_router(models.router)
app.include_router(transcriptions.router)


# æ ¹è·¯å¾‘
@app.get(
    "/",
    summary="API æ ¹è·¯å¾‘",
    description="æä¾› API åŸºæœ¬è³‡è¨Šå’Œå¿«é€Ÿé€£çµ",
    response_description="API è³‡è¨Š",
)
async def root() -> JSONResponse:
    """
    API æ ¹è·¯å¾‘ç«¯é»

    æä¾› API çš„åŸºæœ¬è³‡è¨Šã€ç‰ˆæœ¬å’Œå¿«é€Ÿé€£çµ
    """
    api_info = {
        "name": "MLX Whisper API",
        "version": "1.0.0",
        "description": "OpenAI ç›¸å®¹çš„ MLX Whisper è½‰éŒ„æœå‹™",
        "status": "healthy" if _startup_complete else "starting",
        "endpoints": {
            "health": "/health",
            "models": "/v1/models",
            "transcriptions": "/v1/audio/transcriptions",
            "docs": "/docs",
            "redoc": "/redoc",
            "openapi": "/openapi.json",
        },
        "features": [
            "éŸ³é »è½‰éŒ„ (OpenAI Whisper API ç›¸å®¹)",
            "å¤šèªè¨€æ”¯æ´ (80+ èªè¨€)",
            "å¤šç¨®è¼¸å‡ºæ ¼å¼ (JSON, Text, SRT, VTT)",
            "Apple MLX åŠ é€Ÿ",
            "å‹•æ…‹æ¨¡å‹è¼‰å…¥",
            "é€Ÿç‡é™åˆ¶",
            "Prometheus ç›£æ§",
        ],
        "supported_models": settings.get_supported_models(),
        "max_file_size_mb": settings.get_max_file_size_mb(),
        "timestamp": time.time(),
    }

    return JSONResponse(
        content=api_info,
        headers={"Cache-Control": "public, max-age=300", "X-API-Version": "1.0.0"},
    )


# è‡ªå®šç¾© OpenAPI é…ç½®
def custom_openapi():
    """è‡ªå®šç¾© OpenAPI è¦æ ¼"""
    if app.openapi_schema:
        return app.openapi_schema

    openapi_schema = get_openapi(
        title="MLX Whisper API",
        version="1.0.0",
        description=app.description,
        routes=app.routes,
    )

    # æ·»åŠ æœå‹™å™¨è³‡è¨Š
    openapi_schema["servers"] = [
        {
            "url": f"http://{settings.host}:{settings.port}",
            "description": "æœ¬åœ°é–‹ç™¼ä¼ºæœå™¨",
        },
        {"url": "http://localhost:8000", "description": "é è¨­æœ¬åœ°ä¼ºæœå™¨"},
    ]

    # æ·»åŠ æ¨™ç±¤è³‡è¨Š
    openapi_schema["tags"] = [
        {"name": "health", "description": "å¥åº·æª¢æŸ¥å’Œæœå‹™ç‹€æ…‹"},
        {"name": "models", "description": "æ¨¡å‹ç®¡ç†å’Œè³‡è¨Š"},
        {"name": "transcriptions", "description": "éŸ³é »è½‰éŒ„æœå‹™"},
    ]

    # æ·»åŠ å®‰å…¨å®šç¾©ï¼ˆé›–ç„¶æˆ‘å€‘ä¸ä½¿ç”¨ï¼‰
    openapi_schema["components"]["securitySchemes"] = {
        "ApiKeyAuth": {
            "type": "apiKey",
            "in": "header",
            "name": "Authorization",
            "description": "API å¯†é‘°ï¼ˆå¯é¸ï¼Œç›®å‰ä¸éœ€è¦ï¼‰",
        }
    }

    app.openapi_schema = openapi_schema
    return app.openapi_schema


app.openapi = custom_openapi


# å…¨åŸŸç•°å¸¸è™•ç†å™¨
@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """HTTP ç•°å¸¸è™•ç†å™¨"""
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": {
                "message": exc.detail,
                "type": "http_error",
                "code": exc.status_code,
            }
        },
    )


@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """é€šç”¨ç•°å¸¸è™•ç†å™¨"""
    logger.error(f"æœªè™•ç†çš„ç•°å¸¸: {str(exc)}", exc_info=True)

    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={
            "error": {
                "message": "å…§éƒ¨ä¼ºæœå™¨éŒ¯èª¤",
                "type": "internal_error",
                "code": "internal_server_error",
            }
        },
    )


# è‡ªå®šç¾©ä¸­é–“ä»¶ï¼šè«‹æ±‚æ—¥èªŒ
@app.middleware("http")
async def log_requests(request: Request, call_next):
    """è«‹æ±‚æ—¥èªŒä¸­é–“ä»¶"""
    start_time = time.time()

    # è¨˜éŒ„è«‹æ±‚é–‹å§‹
    logger.info(
        f"ğŸ“¥ {request.method} {request.url.path} - "
        f"å®¢æˆ¶ç«¯: {request.client.host if request.client else 'unknown'}"
    )

    # è™•ç†è«‹æ±‚
    response = await call_next(request)

    # è¨ˆç®—è™•ç†æ™‚é–“
    process_time = time.time() - start_time

    # è¨˜éŒ„å›æ‡‰
    logger.info(
        f"ğŸ“¤ {request.method} {request.url.path} - "
        f"ç‹€æ…‹: {response.status_code}, "
        f"è™•ç†æ™‚é–“: {process_time:.3f}s"
    )

    # æ·»åŠ è™•ç†æ™‚é–“æ¨™é ­
    response.headers["X-Process-Time"] = str(process_time)

    return response


# å¦‚æœç›´æ¥åŸ·è¡Œæ­¤æª”æ¡ˆ
if __name__ == "__main__":
    import uvicorn

    logger.info("ğŸš€ ç›´æ¥å•Ÿå‹• MLX Whisper API...")

    uvicorn.run(
        app,
        host=settings.host,
        port=settings.port,
        workers=1,  # ç›´æ¥é‹è¡Œæ™‚ä½¿ç”¨å–® worker
        log_level=settings.log_level.lower(),
        reload=settings.reload,
        access_log=True,
    )
